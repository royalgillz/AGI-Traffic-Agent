{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7636864,
          "sourceType": "datasetVersion",
          "datasetId": 4450438
        },
        {
          "sourceId": 9403208,
          "sourceType": "datasetVersion",
          "datasetId": 5708059
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royalgillz/AGI-Traffic-Agent/blob/main/traffic_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dtrnngc_ua_detrac_dataset_path = kagglehub.dataset_download('dtrnngc/ua-detrac-dataset')\n",
        "raguhudarare_videotraffic_path = kagglehub.dataset_download('raguhudarare/videotraffic')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "RYv5VLdw_pgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8115f257-ed0d-4127-f115-691d2e29363c"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dtrnngc/ua-detrac-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.23G/9.23G [01:23<00:00, 119MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/raguhudarare/videotraffic?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255M/255M [00:01<00:00, 139MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 1: Setup and Unzip Archive Folder**"
      ],
      "metadata": {
        "id": "I2x3s3hR-dgd",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths for Kaggle input dataset\n",
        "base_path = '/root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload'\n",
        "\n",
        "train_image_path = os.path.join(base_path, 'images/train')\n",
        "val_image_path = os.path.join(base_path, 'images/val')\n",
        "\n",
        "train_label_path = os.path.join(base_path, 'labels/train')\n",
        "val_label_path = os.path.join(base_path, 'labels/val')\n",
        "\n",
        "# Print paths to verify\n",
        "print(f\"Train images path: {train_image_path}\")\n",
        "print(f\"Validation images path: {val_image_path}\")\n",
        "print(f\"Train labels path: {train_label_path}\")\n",
        "print(f\"Validation labels path: {val_label_path}\")\n"
      ],
      "metadata": {
        "id": "c_VQdnz7AHws",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82daf2ac-8c4a-4f1a-e1f6-c7f5b3ed05c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images path: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Validation images path: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Train labels path: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Validation labels path: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"UA-DETRAC dataset path:\", dtrnngc_ua_detrac_dataset_path)\n",
        "print(\"VideoTraffic dataset path:\", raguhudarare_videotraffic_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvC5mwdiJdQf",
        "outputId": "df4c2f61-491d-4c5c-bc3e-7bcf4552eecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UA-DETRAC dataset path: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1\n",
            "VideoTraffic dataset path: /root/.cache/kagglehub/datasets/raguhudarare/videotraffic/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Preprocessing with Progress Prints**"
      ],
      "metadata": {
        "id": "pA5cZ5qi-vjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Convert images to JPG if necessary and provide progress feedback\n",
        "def convert_images_to_jpg(folder_path):\n",
        "    total_files = len(os.listdir(folder_path))\n",
        "    for idx, filename in enumerate(os.listdir(folder_path)):\n",
        "        if not filename.endswith('.jpg'):\n",
        "            img = Image.open(os.path.join(folder_path, filename))\n",
        "            img.convert('RGB').save(os.path.join(folder_path, os.path.splitext(filename)[0] + '.jpg'), 'JPEG')\n",
        "            os.remove(os.path.join(folder_path, filename))  # Remove old file\n",
        "        if idx % 5000 == 0:  # Print progress every 5000 images\n",
        "            print(f\"Processed {idx}/{total_files} images in {folder_path}\")\n",
        "\n",
        "print(\"Converting train images...\")\n",
        "convert_images_to_jpg(train_image_path)\n",
        "\n",
        "print(\"Converting validation images...\")\n",
        "convert_images_to_jpg(val_image_path)\n",
        "\n",
        "# Verify annotations format with progress prints\n",
        "def check_labels(label_folder):\n",
        "    total_labels = len(os.listdir(label_folder))\n",
        "    for idx, label_file in enumerate(os.listdir(label_folder)):\n",
        "        with open(os.path.join(label_folder, label_file), 'r') as file:\n",
        "            for line in file.readlines():\n",
        "                values = line.strip().split(' ')\n",
        "                assert len(values) == 5, f\"Incorrect label format in {label_file}\"\n",
        "        if idx % 5000 == 0:  # Print progress every 5000 labels\n",
        "            print(f\"Verified {idx}/{total_labels} label files in {label_folder}\")\n",
        "\n",
        "print(\"Verifying train labels...\")\n",
        "check_labels(train_label_path)\n",
        "\n",
        "print(\"Verifying validation labels...\")\n",
        "check_labels(val_label_path)\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n"
      ],
      "metadata": {
        "id": "MhKnje0z-knL",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3f4eeb-be00-4da3-b018-d83829b891cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting train images...\n",
            "Processed 0/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 5000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 10000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 15000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 20000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 25000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 30000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 35000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 40000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 45000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 50000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 55000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 60000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 65000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 70000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 75000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Processed 80000/83791 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
            "Converting validation images...\n",
            "Processed 0/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 5000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 10000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 15000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 20000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 25000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 30000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 35000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 40000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 45000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 50000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Processed 55000/56340 images in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
            "Verifying train labels...\n",
            "Verified 0/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 5000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 10000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 15000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 20000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 25000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 30000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 35000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 40000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 45000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 50000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 55000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 60000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 65000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 70000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 75000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verified 80000/82085 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train\n",
            "Verifying validation labels...\n",
            "Verified 0/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 5000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 10000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 15000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 20000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 25000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 30000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 35000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 40000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 45000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 50000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Verified 55000/56167 label files in /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/val\n",
            "Data preprocessing complete!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Install YOLOv5**"
      ],
      "metadata": {
        "id": "LZ5jwMhHBVoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install YOLOv5 dependencies\n",
        "!git clone https://github.com/ultralytics/yolov5  # Clone YOLOv5 repository\n",
        "!pip install -r requirements.txt  # Install required Python packages\n",
        "\n",
        "# If you encounter issues, you might need to install additional packages\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "Mifk-6Ue-2iw",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79437206-b249-4804-f816-1860299e7830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17360, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 17360 (delta 32), reused 13 (delta 10), pack-reused 17308 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17360/17360), 16.26 MiB | 8.02 MiB/s, done.\n",
            "Resolving deltas: 100% (11896/11896), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-15T10:12:25.266194Z",
          "iopub.execute_input": "2024-09-15T10:12:25.267105Z",
          "iopub.status.idle": "2024-09-15T10:12:25.274362Z",
          "shell.execute_reply.started": "2024-09-15T10:12:25.267063Z",
          "shell.execute_reply": "2024-09-15T10:12:25.273468Z"
        },
        "trusted": true,
        "id": "-LBdzSe3_pge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b56778d-7a2c-4c52-9dfa-88120278dea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Training YOLOv5**"
      ],
      "metadata": {
        "id": "UMeGCjdNB3fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2fxpn2StLPPj",
        "outputId": "02c02bae-ce9d-444a-cbb2-7888e432df3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the folder named 'Detrac'\n",
        "!mkdir /kaggle/working/Detrac_File\n",
        "# Create the detrac.yaml file with the required structure\n",
        "detrac_yaml_content = \"\"\"\n",
        "train: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/train\n",
        "val: /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/images/val\n",
        "nc: 4 # Number of classes\n",
        "names:\n",
        "  0: \"class0\"\n",
        "  1: \"class1\"\n",
        "  2: \"class2\"\n",
        "  3: \"class3\"\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to a new detrac.yaml file in the Detrac folder\n",
        "with open('/kaggle/working/Detrac_File/detrac.yaml', 'w') as f:\n",
        "    f.write(detrac_yaml_content)\n",
        "\n",
        "print(\"detrac.yaml created successfully!\")\n"
      ],
      "metadata": {
        "id": "Mx2z6bDgDV6f",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "0584362d-13e9-4986-8097-082d795c35ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/kaggle/working/Detrac_File’: No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/working/Detrac_File/detrac.yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-96df7cdb2814>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Write the content to a new detrac.yaml file in the Detrac folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/Detrac_File/detrac.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetrac_yaml_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/Detrac_File/detrac.yaml'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 10 --data /kaggle/working/Detrac_File/detrac.yaml --weights yolov5s.pt"
      ],
      "metadata": {
        "id": "UOALDZtAB1j2",
        "execution": {
          "iopub.status.busy": "2024-09-15T10:12:30.842196Z",
          "iopub.execute_input": "2024-09-15T10:12:30.842654Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3156c5f6-8821-4e54-d8db-4ad371bc7424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.98-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.98-py3-none-any.whl (949 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.0/950.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.98 ultralytics-thop-2.0.14\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2025-03-29 18:34:03.535673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743273243.795922    7039 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743273243.858192    7039 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msehajgill\u001b[0m (\u001b[33msehajgill3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/kaggle/working/Detrac_File/detrac.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['thop>=0.1.1'] not found, attempting AutoUpdate...\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop>=0.1.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop>=0.1.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop>=0.1.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop>=0.1.1) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.2s, installed 1 package: ['thop>=0.1.1']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 v7.0-411-gf4d8a84c Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/yolov5/wandb/run-20250329_183708-1cpg2cza\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvisionary-dream-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sehajgill3/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sehajgill3/YOLOv5/runs/1cpg2cza\u001b[0m\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 35.5MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 258MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "/content/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0m1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.8, 1.0), 'ra...'mask_interpolation': 0}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/kagglehub/datasets/dtrnngc/ua-detrac-dataset/versions/1/content/UA-DETRAC/DETRAC_Upload/labels/train... 47957 images, 1311 backgrounds, 0 corrupt:  59% 49268/83791 [01:44<01:12, 473.94it/s]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **5. Evaluate the Model**"
      ],
      "metadata": {
        "id": "tji70y0N_pgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /kaggle/working/yolov5/runs/train/exp3/weights/best.pt --data /kaggle/working/Detrac_File/detrac.yaml --img 416"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-15T12:25:11.206639Z",
          "iopub.execute_input": "2024-09-15T12:25:11.207662Z",
          "iopub.status.idle": "2024-09-15T12:37:52.478119Z",
          "shell.execute_reply.started": "2024-09-15T12:25:11.207592Z",
          "shell.execute_reply": "2024-09-15T12:37:52.476224Z"
        },
        "trusted": true,
        "id": "L0see03G_pgf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Inference on Videos**"
      ],
      "metadata": {
        "id": "5PfkFnJa_pgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time  # Added for FPS calculation\n",
        "\n",
        "# Ensure matplotlib works in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the trained model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "\n",
        "# Define the path to the video\n",
        "video_path = '/kaggle/input/videotraffic/Traffic.mp4'  # Replace with your video path\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "frame_count = 0\n",
        "total_vehicles = 0  # Initialize total vehicle counter\n",
        "\n",
        "# Variables for FPS calculation\n",
        "start_time = time.time()\n",
        "fps = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video.\")\n",
        "        break\n",
        "\n",
        "    # Start timing for this frame processing\n",
        "    frame_start_time = time.time()\n",
        "\n",
        "    # Perform inference\n",
        "    results = model(frame)\n",
        "\n",
        "    # Get the number of vehicles detected in the current frame\n",
        "    vehicle_count = len(results.xyxy[0])\n",
        "    total_vehicles += vehicle_count  # Update the total vehicle count\n",
        "\n",
        "    # Calculate FPS\n",
        "    frame_time = time.time() - frame_start_time\n",
        "    fps = 1 / frame_time\n",
        "\n",
        "    # Convert BGR to RGB for matplotlib display\n",
        "    img = cv2.cvtColor(np.squeeze(results.render()), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Add FPS to the image\n",
        "    cv2.putText(img, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Display the frame with YOLOv5 detections\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Frame {frame_count}: {vehicle_count} vehicles detected | FPS: {fps:.2f}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()  # Ensure the plot is shown\n",
        "\n",
        "    # Clear the plot to avoid overlap in subsequent iterations\n",
        "    plt.clf()\n",
        "\n",
        "    frame_count += 1\n",
        "    if frame_count >= 10:  # Display first 10 frames for example\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Calculate average FPS\n",
        "end_time = time.time()\n",
        "avg_fps = frame_count / (end_time - start_time)\n",
        "print(f\"Total vehicles detected: {total_vehicles}\")\n",
        "print(f\"Average FPS: {avg_fps:.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-15T13:20:49.485848Z",
          "iopub.execute_input": "2024-09-15T13:20:49.486781Z",
          "iopub.status.idle": "2024-09-15T13:20:56.258404Z",
          "shell.execute_reply.started": "2024-09-15T13:20:49.486724Z",
          "shell.execute_reply": "2024-09-15T13:20:56.257401Z"
        },
        "trusted": true,
        "id": "_mo7NCLx_pgf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example Code to Simulate Traffic Signal Management:**"
      ],
      "metadata": {
        "id": "BDwhxB0s_pgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate traffic signals for 4 roads\n",
        "traffic_signals = {\n",
        "    'road_1': {'vehicles': 0, 'signal': 'RED'},\n",
        "    'road_2': {'vehicles': 0, 'signal': 'RED'},\n",
        "    'road_3': {'vehicles': 0, 'signal': 'RED'},\n",
        "    'road_4': {'vehicles': 0, 'signal': 'RED'}\n",
        "}\n",
        "\n",
        "# Function to update traffic lights based on vehicle count\n",
        "def update_traffic_signals():\n",
        "    print(\"\\nUpdating Traffic Signals...\")\n",
        "    # Sort roads by vehicle count (most to least vehicles)\n",
        "    sorted_roads = sorted(traffic_signals, key=lambda x: traffic_signals[x]['vehicles'], reverse=True)\n",
        "\n",
        "    # Set the first road with the most vehicles to green, others to red\n",
        "    for road in traffic_signals:\n",
        "        if road == sorted_roads[0]:\n",
        "            traffic_signals[road]['signal'] = 'GREEN'\n",
        "        else:\n",
        "            traffic_signals[road]['signal'] = 'RED'\n",
        "\n",
        "    print(\"Traffic Signal Update:\")\n",
        "    for road, info in traffic_signals.items():\n",
        "        print(f\"  {road}: Signal = {info['signal']}, Vehicles = {info['vehicles']}\")\n",
        "\n",
        "# Load the trained model\n",
        "print(\"Loading YOLOv5 model...\")\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Simulate each road with its own video feed\n",
        "video_paths = {\n",
        "    'road_1': '/kaggle/input/videotraffic/Traffic.mp4',\n",
        "    'road_2': '/kaggle/input/videotraffic/Traffic2.mp4',\n",
        "    'road_3': '/kaggle/input/videotraffic/Traffic3.mp4',\n",
        "    'road_4': '/kaggle/input/videotraffic/Traffic4.mp4'\n",
        "}\n",
        "\n",
        "# Initialize video captures\n",
        "caps = {road: cv2.VideoCapture(path) for road, path in video_paths.items()}\n",
        "\n",
        "# Function to skip frames based on the video FPS and the required skip time (3 seconds)\n",
        "def skip_frames(caps, skip_time):\n",
        "    for road, cap in caps.items():\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        skip_frames = int(fps * skip_time)\n",
        "        for _ in range(skip_frames):\n",
        "            cap.grab()\n",
        "\n",
        "# Process video frames\n",
        "skip_time = 3  # Skip 3 seconds of video\n",
        "iterations = 10  # Number of iterations\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    print(f\"\\nIteration {iteration + 1}\")\n",
        "    for road, cap in caps.items():\n",
        "        # Read a frame from the video feed\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"  End of video for {road}\")\n",
        "            continue\n",
        "\n",
        "        # Perform inference to detect vehicles\n",
        "        print(f\"  Detecting vehicles on {road}...\")\n",
        "        results = model(frame)\n",
        "        vehicle_count = len(results.xyxy[0])\n",
        "\n",
        "        # Update vehicle count for the road\n",
        "        traffic_signals[road]['vehicles'] = vehicle_count\n",
        "        print(f\"  {road}: {vehicle_count} vehicles detected\")\n",
        "\n",
        "        # Render the results on the frame\n",
        "        rendered_frame = results.render()[0]  # Render the bounding boxes and labels\n",
        "\n",
        "        # Convert BGR frame to RGB for Matplotlib\n",
        "        rendered_frame_rgb = cv2.cvtColor(rendered_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the frame using Matplotlib\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(rendered_frame_rgb)\n",
        "        plt.title(f'{road} Frame with Detection')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    # Skip frames equivalent to 3 seconds for each video\n",
        "    skip_frames(caps, skip_time)\n",
        "\n",
        "    # Update traffic signals based on vehicle count\n",
        "    update_traffic_signals()\n",
        "\n",
        "    # Introduce a delay of 1 second between each iteration\n",
        "    time.sleep(1)  # Delay of 1 second before processing the next frame\n",
        "\n",
        "# Release video captures\n",
        "for cap in caps.values():\n",
        "    cap.release()\n",
        "\n",
        "print(\"End of program\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-15T15:20:54.506136Z",
          "iopub.execute_input": "2024-09-15T15:20:54.506553Z"
        },
        "trusted": true,
        "id": "0XSoNo7X_pgf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 2"
      ],
      "metadata": {
        "id": "jiQrGpzxCkaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary tracking package\n",
        "!pip install deep-sort-realtime\n",
        "\n",
        "# Add the Multi-Object Tracking cell\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Ensure matplotlib works in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Initialize DeepSORT tracker\n",
        "tracker = DeepSort(\n",
        "    max_age=30,              # Maximum number of frames to keep track when no detection\n",
        "    n_init=3,                # Number of consecutive detections before track is confirmed\n",
        "    nn_budget=100,           # Maximum size of appearance descriptors gallery\n",
        "    embedder=\"mobilenet\",    # Feature extractor model\n",
        "    nms_max_overlap=1.0,     # Non-max suppression threshold (1.0 = no NMS)\n",
        "    max_cosine_distance=0.2  # Threshold for appearance feature comparison\n",
        ")\n",
        "\n",
        "# Dictionary to store trajectories for each track ID\n",
        "trajectories = defaultdict(list)\n",
        "\n",
        "# Dictionary to store speed for each track ID\n",
        "speeds = defaultdict(float)\n",
        "\n",
        "# Function to calculate Euclidean distance between two points\n",
        "def calculate_distance(point1, point2):\n",
        "    return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
        "\n",
        "# Function to estimate vehicle speed based on pixel displacement\n",
        "def estimate_speed(traj_points, fps):\n",
        "    if len(traj_points) < 5:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate displacement over last 5 points\n",
        "    distances = []\n",
        "    for i in range(1, min(6, len(traj_points))):\n",
        "        distances.append(calculate_distance(traj_points[-i], traj_points[-i-1]))\n",
        "\n",
        "    # Average displacement per frame\n",
        "    avg_displacement = sum(distances) / len(distances)\n",
        "\n",
        "    # Convert to speed (pixels per second)\n",
        "    speed = avg_displacement * fps\n",
        "\n",
        "    return speed\n",
        "\n",
        "# Load the trained model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "\n",
        "# Define the path to the video\n",
        "video_path = '/kaggle/input/videotraffic/Traffic.mp4'\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(f\"Video FPS: {fps}, Resolution: {frame_width}x{frame_height}\")\n",
        "\n",
        "# Variables for tracking statistics\n",
        "frame_count = 0\n",
        "total_vehicles = 0\n",
        "total_unique_vehicles = 0\n",
        "processing_times = []\n",
        "\n",
        "# For visualization\n",
        "colors = np.random.randint(0, 255, size=(100, 3), dtype=np.uint8)\n",
        "\n",
        "# Create a video writer (optional, if you want to save the output)\n",
        "# out = cv2.VideoWriter('tracked_output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "# Process video frames\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video.\")\n",
        "        break\n",
        "\n",
        "    # Start timing for this frame processing\n",
        "    frame_start_time = time.time()\n",
        "\n",
        "    # Perform detection inference\n",
        "    results = model(frame)\n",
        "\n",
        "    # Get detections as numpy array [x1, y1, x2, y2, confidence, class]\n",
        "    detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "    # Format detections for DeepSORT [x1, y1, width, height, confidence, class]\n",
        "    detection_list = []\n",
        "    for detection in detections:\n",
        "        x1, y1, x2, y2, conf, cls = detection\n",
        "        detection_list.append(\n",
        "            ([x1, y1, x2 - x1, y2 - y1], conf, int(cls))\n",
        "        )\n",
        "\n",
        "    # Update tracker with detections\n",
        "    tracks = tracker.update_tracks(detection_list, frame=frame)\n",
        "\n",
        "    # Count active tracks in this frame\n",
        "    active_tracks = [track for track in tracks if track.is_confirmed()]\n",
        "    frame_vehicle_count = len(active_tracks)\n",
        "    total_vehicles += frame_vehicle_count\n",
        "\n",
        "    # Keep track of unique vehicles\n",
        "    for track in active_tracks:\n",
        "        track_id = track.track_id\n",
        "        if track_id not in trajectories:\n",
        "            total_unique_vehicles += 1\n",
        "\n",
        "        # Get track position (center of bounding box)\n",
        "        bbox = track.to_ltrb()\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        center_x = int((x1 + x2) / 2)\n",
        "        center_y = int((y1 + y2) / 2)\n",
        "\n",
        "        # Add point to trajectory\n",
        "        trajectories[track_id].append((center_x, center_y))\n",
        "\n",
        "        # Update speed estimate\n",
        "        speeds[track_id] = estimate_speed(trajectories[track_id], fps)\n",
        "\n",
        "    # Draw tracking results on the frame\n",
        "    tracked_frame = frame.copy()\n",
        "\n",
        "    for track in active_tracks:\n",
        "        track_id = track.track_id\n",
        "        bbox = track.to_ltrb()\n",
        "        x1, y1, x2, y2 = bbox\n",
        "\n",
        "        # Get color for this track ID\n",
        "        color = colors[track_id % len(colors)].tolist()\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(tracked_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "\n",
        "        # Add ID and speed text\n",
        "        speed_text = f\"ID:{track_id} Speed:{speeds[track_id]:.1f}\"\n",
        "        cv2.putText(tracked_frame, speed_text, (int(x1), int(y1)-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # Draw trajectory\n",
        "        if len(trajectories[track_id]) > 1:\n",
        "            # Draw the trajectory line\n",
        "            for i in range(1, len(trajectories[track_id])):\n",
        "                pt1 = trajectories[track_id][i-1]\n",
        "                pt2 = trajectories[track_id][i]\n",
        "                cv2.line(tracked_frame, pt1, pt2, color, 2)\n",
        "\n",
        "    # Calculate FPS\n",
        "    processing_time = time.time() - frame_start_time\n",
        "    processing_times.append(processing_time)\n",
        "    fps_current = 1.0 / processing_time if processing_time > 0 else 0\n",
        "\n",
        "    # Add FPS text to the frame\n",
        "    cv2.putText(tracked_frame, f\"FPS: {fps_current:.2f}\", (10, 30),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Add detection and tracking info\n",
        "    cv2.putText(tracked_frame, f\"Tracking: {frame_vehicle_count} vehicles\", (10, 60),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Convert BGR to RGB for matplotlib display\n",
        "    tracked_frame_rgb = cv2.cvtColor(tracked_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the frame\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(tracked_frame_rgb)\n",
        "    plt.title(f\"Frame {frame_count}: {frame_vehicle_count} vehicles tracked | FPS: {fps_current:.2f}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "    # Optional: Save frame to output video\n",
        "    # out.write(tracked_frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # For demo purposes, process a limited number of frames\n",
        "    if frame_count >= 20:  # Increase this to process more frames\n",
        "        break\n",
        "\n",
        "# Release video and cleanup\n",
        "cap.release()\n",
        "# out.release()  # Uncomment if saving video\n",
        "\n",
        "# Calculate and display performance metrics\n",
        "avg_processing_time = sum(processing_times) / len(processing_times)\n",
        "avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "print(f\"Processed {frame_count} frames\")\n",
        "print(f\"Average processing time: {avg_processing_time:.3f} seconds per frame\")\n",
        "print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "print(f\"Total vehicles tracked: {total_unique_vehicles}\")\n",
        "print(f\"Average vehicles per frame: {total_vehicles / frame_count:.2f}\")\n",
        "\n",
        "# Generate trajectory visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Vehicle Trajectories\")\n",
        "\n",
        "# Create a blended background (optional, average of first and last frame)\n",
        "background = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
        "# Background could be created by blending first/last frames if needed\n",
        "\n",
        "# Draw all trajectories on a single image\n",
        "for track_id, traj in trajectories.items():\n",
        "    if len(traj) < 3:  # Skip very short trajectories\n",
        "        continue\n",
        "\n",
        "    # Get color for this track\n",
        "    color = colors[track_id % len(colors)].tolist()\n",
        "\n",
        "    # Convert to numpy for easier indexing\n",
        "    traj_np = np.array(traj)\n",
        "\n",
        "    # Plot trajectory points\n",
        "    plt.plot(traj_np[:, 0], traj_np[:, 1], '-', color=[c/255.0 for c in color], linewidth=2,\n",
        "             label=f\"Vehicle {track_id}\")\n",
        "\n",
        "    # Mark start and end points\n",
        "    plt.plot(traj_np[0, 0], traj_np[0, 1], 'o', color=[c/255.0 for c in color], markersize=8)\n",
        "    plt.plot(traj_np[-1, 0], traj_np[-1, 1], 's', color=[c/255.0 for c in color], markersize=8)\n",
        "\n",
        "# Set plot properties\n",
        "plt.xlim(0, frame_width)\n",
        "plt.ylim(frame_height, 0)  # Invert y-axis to match image coordinates\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Advanced: Analyze trajectories for patterns\n",
        "# Calculate average direction and speed for each trajectory\n",
        "trajectory_stats = {}\n",
        "\n",
        "for track_id, traj in trajectories.items():\n",
        "    if len(traj) < 5:  # Need enough points for meaningful analysis\n",
        "        continue\n",
        "\n",
        "    # Calculate overall displacement vector\n",
        "    start_point = np.array(traj[0])\n",
        "    end_point = np.array(traj[-1])\n",
        "    displacement = end_point - start_point\n",
        "\n",
        "    # Calculate direction angle in degrees (0 is right, 90 is up)\n",
        "    direction = math.degrees(math.atan2(-displacement[1], displacement[0]))  # Negative Y for image coordinates\n",
        "\n",
        "    # Calculate average speed\n",
        "    total_distance = 0\n",
        "    for i in range(1, len(traj)):\n",
        "        total_distance += calculate_distance(traj[i-1], traj[i])\n",
        "\n",
        "    avg_speed = total_distance / len(traj)\n",
        "\n",
        "    # Store stats\n",
        "    trajectory_stats[track_id] = {\n",
        "        'direction': direction,\n",
        "        'avg_speed': avg_speed,\n",
        "        'displacement': np.linalg.norm(displacement),\n",
        "        'trajectory_length': len(traj)\n",
        "    }\n",
        "\n",
        "# Print trajectory statistics\n",
        "print(\"\\nTrajectory Statistics:\")\n",
        "for track_id, stats in trajectory_stats.items():\n",
        "    print(f\"Vehicle {track_id}:\")\n",
        "    print(f\"  Direction: {stats['direction']:.1f} degrees\")\n",
        "    print(f\"  Average Speed: {stats['avg_speed']:.1f} pixels/frame\")\n",
        "    print(f\"  Displacement: {stats['displacement']:.1f} pixels\")\n",
        "    print(f\"  Tracked for {stats['trajectory_length']} frames\")"
      ],
      "metadata": {
        "id": "-wt8RRFuCeLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 3"
      ],
      "metadata": {
        "id": "Tu-Sr96AlCUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Ensure matplotlib works in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Step 1: Define behavior analysis functions\n",
        "def analyze_trajectory_direction(trajectory, window_size=10):\n",
        "    \"\"\"Analyze trajectory to determine if vehicle is turning and in which direction\"\"\"\n",
        "    if len(trajectory) < window_size:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Get recent trajectory points\n",
        "    recent_traj = trajectory[-window_size:]\n",
        "\n",
        "    # Calculate direction changes\n",
        "    angles = []\n",
        "    for i in range(1, len(recent_traj)):\n",
        "        dx = recent_traj[i][0] - recent_traj[i-1][0]\n",
        "        dy = recent_traj[i][1] - recent_traj[i-1][1]\n",
        "        angle = math.atan2(dy, dx)\n",
        "        angles.append(angle)\n",
        "\n",
        "    # Calculate angle difference between start and end\n",
        "    angle_diff = abs(angles[-1] - angles[0])\n",
        "\n",
        "    # Classify turn direction\n",
        "    if angle_diff > 0.3:  # Threshold for turn detection (in radians)\n",
        "        # Determine turn direction\n",
        "        if (angles[-1] - angles[0]) > 0:\n",
        "            return \"Turning Left\"\n",
        "        else:\n",
        "            return \"Turning Right\"\n",
        "    else:\n",
        "        return \"Going Straight\"\n",
        "\n",
        "def detect_red_light_violation(trajectory, current_pos, stop_line_y, light_state):\n",
        "    \"\"\"Detect if a vehicle might run a red light\"\"\"\n",
        "    if light_state != \"RED\" or len(trajectory) < 5:\n",
        "        return False\n",
        "\n",
        "    # Check if vehicle is approaching the stop line\n",
        "    if current_pos[1] < stop_line_y and current_pos[1] > stop_line_y - 100:\n",
        "        # Calculate speed\n",
        "        recent_traj = trajectory[-5:]\n",
        "        distances = []\n",
        "        for i in range(1, len(recent_traj)):\n",
        "            distances.append(calculate_distance(recent_traj[i], recent_traj[i-1]))\n",
        "\n",
        "        avg_speed = sum(distances) / len(distances)\n",
        "\n",
        "        # If approaching fast and not slowing down\n",
        "        if avg_speed > 5.0:  # Threshold for \"fast\" approach\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Function to calculate Euclidean distance between two points\n",
        "def calculate_distance(point1, point2):\n",
        "    return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
        "\n",
        "# Function to estimate vehicle speed based on pixel displacement\n",
        "def estimate_speed(traj_points, fps):\n",
        "    if len(traj_points) < 5:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate displacement over last 5 points\n",
        "    distances = []\n",
        "    for i in range(1, min(6, len(traj_points))):\n",
        "        distances.append(calculate_distance(traj_points[-i], traj_points[-i-1]))\n",
        "\n",
        "    # Average displacement per frame\n",
        "    avg_displacement = sum(distances) / len(distances)\n",
        "\n",
        "    # Convert to speed (pixels per second)\n",
        "    speed = avg_displacement * fps\n",
        "\n",
        "    return speed\n",
        "\n",
        "# Step 2: Define intersection zones\n",
        "def define_intersection_zones(frame_width, frame_height):\n",
        "    \"\"\"Define virtual zones for traffic analysis based on frame dimensions\"\"\"\n",
        "\n",
        "    # Example for a four-way intersection\n",
        "    intersection = {\n",
        "        'center': (frame_width//2, frame_height//2),\n",
        "        'radius': 150,\n",
        "        'stop_lines': {\n",
        "            'north': frame_height//2 - 100,\n",
        "            'south': frame_height//2 + 100,\n",
        "            'east': frame_width//2 + 100,\n",
        "            'west': frame_width//2 - 100\n",
        "        },\n",
        "        'light_states': {\n",
        "            'north': 'RED',\n",
        "            'south': 'RED',\n",
        "            'east': 'GREEN',\n",
        "            'west': 'RED'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return intersection\n",
        "\n",
        "# Function to visualize intersection zones\n",
        "def draw_intersection_zones(frame, intersection):\n",
        "    \"\"\"Draw intersection zones on the frame\"\"\"\n",
        "    frame_width = frame.shape[1]\n",
        "    frame_height = frame.shape[0]\n",
        "\n",
        "    # Draw center circle\n",
        "    cv2.circle(frame, intersection['center'], 10, (0, 0, 255), -1)\n",
        "    cv2.circle(frame, intersection['center'], intersection['radius'], (0, 0, 255), 2)\n",
        "\n",
        "    # Draw stop lines\n",
        "    cv2.line(frame,\n",
        "             (0, intersection['stop_lines']['north']),\n",
        "             (frame_width, intersection['stop_lines']['north']),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (0, intersection['stop_lines']['south']),\n",
        "             (frame_width, intersection['stop_lines']['south']),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (intersection['stop_lines']['west'], 0),\n",
        "             (intersection['stop_lines']['west'], frame_height),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (intersection['stop_lines']['east'], 0),\n",
        "             (intersection['stop_lines']['east'], frame_height),\n",
        "             (255, 0, 0), 2)\n",
        "\n",
        "    # Add traffic light indicators\n",
        "    light_positions = {\n",
        "        'north': (frame_width//2 - 50, intersection['stop_lines']['north'] - 20),\n",
        "        'south': (frame_width//2 + 50, intersection['stop_lines']['south'] + 20),\n",
        "        'east': (intersection['stop_lines']['east'] + 20, frame_height//2 - 50),\n",
        "        'west': (intersection['stop_lines']['west'] - 20, frame_height//2 + 50)\n",
        "    }\n",
        "\n",
        "    for direction, position in light_positions.items():\n",
        "        # Determine light color\n",
        "        if intersection['light_states'][direction] == 'RED':\n",
        "            light_color = (0, 0, 255)  # Red\n",
        "        elif intersection['light_states'][direction] == 'YELLOW':\n",
        "            light_color = (0, 255, 255)  # Yellow\n",
        "        else:  # GREEN\n",
        "            light_color = (0, 255, 0)  # Green\n",
        "\n",
        "        # Draw light indicator\n",
        "        cv2.circle(frame, position, 15, light_color, -1)\n",
        "        cv2.circle(frame, position, 15, (255, 255, 255), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Function to update traffic light states\n",
        "def update_traffic_lights(intersection, frame_count, cycle_length=150):\n",
        "    \"\"\"Update traffic light states based on a simple cycle\"\"\"\n",
        "    # Simple traffic light cycle: N/S green, E/W red, then switch\n",
        "    cycle_position = frame_count % cycle_length\n",
        "\n",
        "    # First half of cycle: N/S green, E/W red\n",
        "    if cycle_position < cycle_length // 2:\n",
        "        intersection['light_states']['north'] = 'GREEN'\n",
        "        intersection['light_states']['south'] = 'GREEN'\n",
        "        intersection['light_states']['east'] = 'RED'\n",
        "        intersection['light_states']['west'] = 'RED'\n",
        "\n",
        "        # Add yellow transition\n",
        "        if cycle_position > (cycle_length // 2) - 30:\n",
        "            intersection['light_states']['north'] = 'YELLOW'\n",
        "            intersection['light_states']['south'] = 'YELLOW'\n",
        "    # Second half: N/S red, E/W green\n",
        "    else:\n",
        "        intersection['light_states']['north'] = 'RED'\n",
        "        intersection['light_states']['south'] = 'RED'\n",
        "        intersection['light_states']['east'] = 'GREEN'\n",
        "        intersection['light_states']['west'] = 'GREEN'\n",
        "\n",
        "        # Add yellow transition\n",
        "        if cycle_position > cycle_length - 30:\n",
        "            intersection['light_states']['east'] = 'YELLOW'\n",
        "            intersection['light_states']['west'] = 'YELLOW'\n",
        "\n",
        "    return intersection\n",
        "\n",
        "# Step 3-5: Main tracking and behavior prediction function\n",
        "def track_and_predict_behavior(video_path, model, max_frames=50):\n",
        "    \"\"\"Main function to track vehicles and predict their behavior\"\"\"\n",
        "\n",
        "    # Initialize DeepSORT tracker\n",
        "    tracker = DeepSort(\n",
        "        max_age=30,              # Maximum number of frames to keep track when no detection\n",
        "        n_init=3,                # Number of consecutive detections before track is confirmed\n",
        "        nn_budget=100,           # Maximum size of appearance descriptors gallery\n",
        "        embedder=\"mobilenet\",    # Feature extractor model\n",
        "        nms_max_overlap=1.0,     # Non-max suppression threshold (1.0 = no NMS)\n",
        "        max_cosine_distance=0.2  # Threshold for appearance feature comparison\n",
        "    )\n",
        "\n",
        "    # Dictionary to store trajectories for each track ID\n",
        "    trajectories = defaultdict(list)\n",
        "\n",
        "    # Dictionary to store speed for each track ID\n",
        "    speeds = defaultdict(float)\n",
        "\n",
        "    # Dictionary to store behavior for each track ID\n",
        "    behaviors = defaultdict(str)\n",
        "\n",
        "    # Dictionary to store violations for each track ID\n",
        "    violations = defaultdict(bool)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"Video FPS: {fps}, Resolution: {frame_width}x{frame_height}\")\n",
        "\n",
        "    # Define intersection zones\n",
        "    intersection = define_intersection_zones(frame_width, frame_height)\n",
        "\n",
        "    # Variables for tracking statistics\n",
        "    frame_count = 0\n",
        "    total_vehicles = 0\n",
        "    total_unique_vehicles = 0\n",
        "    total_violations = 0\n",
        "    processing_times = []\n",
        "\n",
        "    # For visualization\n",
        "    colors = np.random.randint(0, 255, size=(100, 3), dtype=np.uint8)\n",
        "\n",
        "    # Process video frames\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"End of video.\")\n",
        "            break\n",
        "\n",
        "        # Start timing for this frame processing\n",
        "        frame_start_time = time.time()\n",
        "\n",
        "        # Update traffic light states\n",
        "        intersection = update_traffic_lights(intersection, frame_count)\n",
        "\n",
        "        # Perform detection inference\n",
        "        results = model(frame)\n",
        "\n",
        "        # Get detections as numpy array [x1, y1, x2, y2, confidence, class]\n",
        "        detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "        # Format detections for DeepSORT [x1, y1, width, height, confidence, class]\n",
        "        detection_list = []\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            detection_list.append(\n",
        "                ([x1, y1, x2 - x1, y2 - y1], conf, int(cls))\n",
        "            )\n",
        "\n",
        "        # Update tracker with detections\n",
        "        tracks = tracker.update_tracks(detection_list, frame=frame)\n",
        "\n",
        "        # Count active tracks in this frame\n",
        "        active_tracks = [track for track in tracks if track.is_confirmed()]\n",
        "        frame_vehicle_count = len(active_tracks)\n",
        "        total_vehicles += frame_vehicle_count\n",
        "\n",
        "        # Draw tracking results on the frame\n",
        "        tracked_frame = frame.copy()\n",
        "\n",
        "        # Draw intersection zones\n",
        "        tracked_frame = draw_intersection_zones(tracked_frame, intersection)\n",
        "\n",
        "        # Keep track of unique vehicles and update trajectories\n",
        "        for track in active_tracks:\n",
        "            track_id = track.track_id\n",
        "            bbox = track.to_ltrb()\n",
        "            x1, y1, x2, y2 = bbox\n",
        "\n",
        "            # Get track position (center of bounding box)\n",
        "            center_x = int((x1 + x2) / 2)\n",
        "            center_y = int((y1 + y2) / 2)\n",
        "            current_pos = (center_x, center_y)\n",
        "\n",
        "            # Add point to trajectory\n",
        "            trajectories[track_id].append(current_pos)\n",
        "\n",
        "            # Update speed estimate\n",
        "            speeds[track_id] = estimate_speed(trajectories[track_id], fps)\n",
        "\n",
        "            # Count unique vehicles\n",
        "            if len(trajectories[track_id]) == 1:\n",
        "                total_unique_vehicles += 1\n",
        "\n",
        "            # Get color for this track ID\n",
        "            color = colors[track_id % len(colors)].tolist()\n",
        "\n",
        "            # Determine which region of the intersection the vehicle is in\n",
        "            if center_x < intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                approach_direction = 'north'\n",
        "            elif center_x >= intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                approach_direction = 'east'\n",
        "            elif center_x < intersection['center'][0] and center_y >= intersection['center'][1]:\n",
        "                approach_direction = 'west'\n",
        "            else:\n",
        "                approach_direction = 'south'\n",
        "\n",
        "            # Step 3: Analyze behavior if we have enough trajectory history\n",
        "            if len(trajectories[track_id]) >= 10:\n",
        "                behaviors[track_id] = analyze_trajectory_direction(trajectories[track_id])\n",
        "\n",
        "                # Step 4: Check for red light violation\n",
        "                stop_line_y = intersection['stop_lines'][approach_direction]\n",
        "                light_state = intersection['light_states'][approach_direction]\n",
        "\n",
        "                # Detect violation\n",
        "                new_violation = detect_red_light_violation(\n",
        "                    trajectories[track_id],\n",
        "                    current_pos,\n",
        "                    stop_line_y,\n",
        "                    light_state\n",
        "                )\n",
        "\n",
        "                # Count new violations\n",
        "                if new_violation and not violations[track_id]:\n",
        "                    total_violations += 1\n",
        "\n",
        "                violations[track_id] = new_violation\n",
        "\n",
        "            # Step 5: Integrate behavior prediction into tracking visualization\n",
        "            # Draw bounding box (red for violation, normal color otherwise)\n",
        "            box_color = (0, 0, 255) if violations[track_id] else color\n",
        "            cv2.rectangle(tracked_frame, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)\n",
        "\n",
        "            # Add ID and behavior text\n",
        "            behavior_text = f\"ID:{track_id} {behaviors[track_id]}\"\n",
        "            cv2.putText(tracked_frame, behavior_text, (int(x1), int(y1)-10),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Add speed text\n",
        "            speed_text = f\"Speed:{speeds[track_id]:.1f}\"\n",
        "            cv2.putText(tracked_frame, speed_text, (int(x1), int(y1)-30),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Add violation warning\n",
        "            if violations[track_id]:\n",
        "                cv2.putText(tracked_frame, \"VIOLATION!\", (int(x1), int(y1)-50),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "            # Draw trajectory\n",
        "            if len(trajectories[track_id]) > 1:\n",
        "                # Draw the trajectory line\n",
        "                for i in range(1, len(trajectories[track_id])):\n",
        "                    pt1 = trajectories[track_id][i-1]\n",
        "                    pt2 = trajectories[track_id][i]\n",
        "                    cv2.line(tracked_frame, pt1, pt2, color, 2)\n",
        "\n",
        "        # Calculate processing time and FPS\n",
        "        processing_time = time.time() - frame_start_time\n",
        "        processing_times.append(processing_time)\n",
        "        fps_current = 1.0 / processing_time if processing_time > 0 else 0\n",
        "\n",
        "        # Add FPS text to the frame\n",
        "        cv2.putText(tracked_frame, f\"FPS: {fps_current:.2f}\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add detection and tracking info\n",
        "        cv2.putText(tracked_frame, f\"Tracking: {frame_vehicle_count} vehicles\", (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add violation counter\n",
        "        cv2.putText(tracked_frame, f\"Violations: {total_violations}\", (10, 90),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Convert BGR to RGB for matplotlib display\n",
        "        tracked_frame_rgb = cv2.cvtColor(tracked_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the frame\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(tracked_frame_rgb)\n",
        "        plt.title(f\"Frame {frame_count}: {frame_vehicle_count} vehicles tracked | FPS: {fps_current:.2f}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        plt.clf()\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # For demo purposes, process a limited number of frames\n",
        "        if frame_count >= max_frames:  # Increase this to process more frames\n",
        "            break\n",
        "\n",
        "    # Release video\n",
        "    cap.release()\n",
        "\n",
        "    # Calculate and display performance metrics\n",
        "    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0\n",
        "    avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "    print(f\"Processed {frame_count} frames\")\n",
        "    print(f\"Average processing time: {avg_processing_time:.3f} seconds per frame\")\n",
        "    print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "    print(f\"Total unique vehicles tracked: {total_unique_vehicles}\")\n",
        "    print(f\"Average vehicles per frame: {total_vehicles / frame_count:.2f}\")\n",
        "    print(f\"Total red light violations detected: {total_violations}\")\n",
        "\n",
        "    # Step 6: Calculate behavior prediction metrics\n",
        "    print(\"\\nBehavior Prediction Statistics:\")\n",
        "    behavior_counts = defaultdict(int)\n",
        "    for track_id, behavior in behaviors.items():\n",
        "        behavior_counts[behavior] += 1\n",
        "\n",
        "    for behavior, count in behavior_counts.items():\n",
        "        print(f\"  {behavior}: {count} vehicles\")\n",
        "\n",
        "    print(f\"\\nViolation rate: {total_violations / total_unique_vehicles * 100:.1f}% of vehicles\")\n",
        "\n",
        "    # Return data for further analysis\n",
        "    return {\n",
        "        'trajectories': trajectories,\n",
        "        'behaviors': behaviors,\n",
        "        'violations': violations,\n",
        "        'speeds': speeds,\n",
        "        'metrics': {\n",
        "            'total_frames': frame_count,\n",
        "            'total_unique_vehicles': total_unique_vehicles,\n",
        "            'total_violations': total_violations,\n",
        "            'avg_fps': avg_fps\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Function to visualize all trajectories with behavior information\n",
        "def visualize_trajectories(trajectory_data, frame_width, frame_height):\n",
        "    \"\"\"Create a summary visualization of all vehicle trajectories and their behaviors\"\"\"\n",
        "\n",
        "    trajectories = trajectory_data['trajectories']\n",
        "    behaviors = trajectory_data['behaviors']\n",
        "    violations = trajectory_data['violations']\n",
        "\n",
        "    # Set up the plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title(\"Vehicle Trajectories and Behaviors\")\n",
        "\n",
        "    # Define colors for different behaviors\n",
        "    behavior_colors = {\n",
        "        \"Going Straight\": \"blue\",\n",
        "        \"Turning Left\": \"green\",\n",
        "        \"Turning Right\": \"purple\",\n",
        "        \"Unknown\": \"gray\"\n",
        "    }\n",
        "\n",
        "    # Draw intersection\n",
        "    intersection = define_intersection_zones(frame_width, frame_height)\n",
        "    plt.plot(intersection['center'][0], intersection['center'][1], 'ro', markersize=10)\n",
        "    plt.plot([0, frame_width], [intersection['stop_lines']['north'], intersection['stop_lines']['north']], 'r--')\n",
        "    plt.plot([0, frame_width], [intersection['stop_lines']['south'], intersection['stop_lines']['south']], 'r--')\n",
        "    plt.plot([intersection['stop_lines']['west'], intersection['stop_lines']['west']], [0, frame_height], 'r--')\n",
        "    plt.plot([intersection['stop_lines']['east'], intersection['stop_lines']['east']], [0, frame_height], 'r--')\n",
        "\n",
        "    # Draw all trajectories on a single image\n",
        "    for track_id, traj in trajectories.items():\n",
        "        if len(traj) < 3:  # Skip very short trajectories\n",
        "            continue\n",
        "\n",
        "        # Get behavior for this track\n",
        "        behavior = behaviors.get(track_id, \"Unknown\")\n",
        "\n",
        "        # Get color based on behavior\n",
        "        color = behavior_colors.get(behavior, \"gray\")\n",
        "\n",
        "        # Mark violations with a red outline\n",
        "        is_violation = violations.get(track_id, False)\n",
        "\n",
        "        # Convert to numpy for easier indexing\n",
        "        traj_np = np.array(traj)\n",
        "\n",
        "        # Plot trajectory\n",
        "        plt.plot(traj_np[:, 0], traj_np[:, 1], '-', color=color, linewidth=2,\n",
        "                 label=f\"Vehicle {track_id}: {behavior}\" if track_id % 5 == 0 else \"\")\n",
        "\n",
        "        # Mark start and end points\n",
        "        plt.plot(traj_np[0, 0], traj_np[0, 1], 'o', color=color, markersize=8)\n",
        "\n",
        "        # Use a red square for the end point of vehicles with violations\n",
        "        if is_violation:\n",
        "            plt.plot(traj_np[-1, 0], traj_np[-1, 1], 's', color='red', markersize=10)\n",
        "        else:\n",
        "            plt.plot(traj_np[-1, 0], traj_np[-1, 1], 's', color=color, markersize=8)\n",
        "\n",
        "    # Create a custom legend for behaviors\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='blue', lw=2, label='Going Straight'),\n",
        "        Line2D([0], [0], color='green', lw=2, label='Turning Left'),\n",
        "        Line2D([0], [0], color='purple', lw=2, label='Turning Right'),\n",
        "        Line2D([0], [0], color='gray', lw=2, label='Unknown'),\n",
        "        Line2D([0], [0], marker='s', color='red', label='Violation', markersize=8, linestyle='None')\n",
        "    ]\n",
        "\n",
        "    # Add legend with behavior types\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "    # Set plot properties\n",
        "    plt.xlim(0, frame_width)\n",
        "    plt.ylim(frame_height, 0)  # Invert y-axis to match image coordinates\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Load the model and run tracking with behavior prediction\n",
        "# model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "# video_path = '/kaggle/input/videotraffic/Traffic.mp4'\n",
        "# results = track_and_predict_behavior(video_path, model, max_frames=30)\n",
        "# visualize_trajectories(results, 1280, 720)  # Use actual video dimensions"
      ],
      "metadata": {
        "id": "wHbwn4Lzk-Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "\n",
        "# Define your video path\n",
        "video_path = '/kaggle/input/videotraffic/Traffic.mp4'\n",
        "\n",
        "# Run tracking with behavior prediction\n",
        "results = track_and_predict_behavior(video_path, model, max_frames=30)\n",
        "\n",
        "# Get video dimensions for visualization\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap.release()\n",
        "\n",
        "# Visualize all trajectories with behavior information\n",
        "visualize_trajectories(results, width, height)"
      ],
      "metadata": {
        "id": "9YLaoFpxlVAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Milestone 4"
      ],
      "metadata": {
        "id": "VdOFmrC7oxIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from scipy.signal import savgol_filter\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from filterpy.common import Q_discrete_white_noise\n",
        "\n",
        "# Ensure matplotlib works in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Step 7: Optimize tracker for occlusion handling\n",
        "def create_optimized_tracker():\n",
        "    \"\"\"Create a DeepSORT tracker with optimized parameters for occlusion handling\"\"\"\n",
        "    tracker = DeepSort(\n",
        "        max_age=45,               # Increased from 30 to keep tracks alive longer during occlusions\n",
        "        n_init=2,                 # Reduced from 3 to confirm tracks faster in difficult scenes\n",
        "        nn_budget=150,            # Increased from 100 to store more appearance features\n",
        "        embedder=\"mobilenet\",     # Feature extractor model\n",
        "        nms_max_overlap=0.8,      # Reduced from 1.0 to apply some non-max suppression\n",
        "        max_cosine_distance=0.3,  # Increased from 0.2 to allow more appearance variation\n",
        "        max_iou_distance=0.7,     # Explicitly set max IOU distance for better occlusion handling\n",
        "        embedder_gpu=True,        # Use GPU for embedding computation if available\n",
        "        embedder_model_name=None, # Use default model\n",
        "        polygon=False             # Use rectangle (default) to represent objects\n",
        "    )\n",
        "    return tracker\n",
        "\n",
        "# Step 9: Implement Kalman Filter for position prediction during occlusion\n",
        "class VehicleKalmanFilter:\n",
        "    \"\"\"Kalman Filter implementation for vehicle position prediction during occlusion\"\"\"\n",
        "    def __init__(self, initial_pos, dt=1.0):\n",
        "        # 6 state variables [x, y, vx, vy, ax, ay], 2 measured variables [x, y]\n",
        "        self.kf = KalmanFilter(dim_x=6, dim_z=2)\n",
        "\n",
        "        # State transition matrix (position + velocity + acceleration model)\n",
        "        self.kf.F = np.array([\n",
        "            [1, 0, dt, 0, 0.5*dt**2, 0],\n",
        "            [0, 1, 0, dt, 0, 0.5*dt**2],\n",
        "            [0, 0, 1, 0, dt, 0],\n",
        "            [0, 0, 0, 1, 0, dt],\n",
        "            [0, 0, 0, 0, 1, 0],\n",
        "            [0, 0, 0, 0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        # Measurement function (we only measure position, not velocity or acceleration)\n",
        "        self.kf.H = np.array([\n",
        "            [1, 0, 0, 0, 0, 0],\n",
        "            [0, 1, 0, 0, 0, 0]\n",
        "        ])\n",
        "\n",
        "        # Measurement noise\n",
        "        self.kf.R = np.array([\n",
        "            [10, 0],\n",
        "            [0, 10]\n",
        "        ])\n",
        "\n",
        "        # Process noise\n",
        "        self.kf.Q = np.eye(6) * 0.1\n",
        "\n",
        "        # Process noise for position, velocity, and acceleration\n",
        "        self.kf.Q[0:2, 0:2] *= 0.01  # position noise\n",
        "        self.kf.Q[2:4, 2:4] *= 0.1   # velocity noise\n",
        "        self.kf.Q[4:6, 4:6] *= 1.0   # acceleration noise\n",
        "\n",
        "        # Initial state\n",
        "        self.kf.x = np.array([initial_pos[0], initial_pos[1], 0, 0, 0, 0]).reshape(6, 1)\n",
        "\n",
        "        # Initial state covariance\n",
        "        self.kf.P *= 100\n",
        "\n",
        "        # Store prediction history\n",
        "        self.predictions = [initial_pos]\n",
        "        self.measurements = [initial_pos]\n",
        "\n",
        "        # Time step\n",
        "        self.dt = dt\n",
        "\n",
        "    def update(self, measurement):\n",
        "        \"\"\"Update the filter with a new measurement\"\"\"\n",
        "        self.kf.predict()\n",
        "        self.kf.update(np.array([measurement[0], measurement[1]]))\n",
        "\n",
        "        # Store measurement and current state\n",
        "        self.measurements.append(measurement)\n",
        "        self.predictions.append((self.kf.x[0, 0], self.kf.x[1, 0]))\n",
        "\n",
        "        return (self.kf.x[0, 0], self.kf.x[1, 0])\n",
        "\n",
        "    def predict(self, steps=1):\n",
        "        \"\"\"Predict position several steps into the future\"\"\"\n",
        "        # Save current state\n",
        "        current_x = self.kf.x.copy()\n",
        "        current_P = self.kf.P.copy()\n",
        "\n",
        "        # Make predictions\n",
        "        for _ in range(steps):\n",
        "            self.kf.predict()\n",
        "\n",
        "        # Get prediction\n",
        "        prediction = (self.kf.x[0, 0], self.kf.x[1, 0])\n",
        "\n",
        "        # Restore state\n",
        "        self.kf.x = current_x\n",
        "        self.kf.P = current_P\n",
        "\n",
        "        return prediction\n",
        "\n",
        "# Function to smooth trajectory using Savitzky-Golay filter\n",
        "def smooth_trajectory(traj, window_size=15, poly_order=3):\n",
        "    \"\"\"Smooth a trajectory using Savitzky-Golay filter\"\"\"\n",
        "    if len(traj) < window_size:\n",
        "        return traj\n",
        "\n",
        "    # Convert to numpy array\n",
        "    traj_np = np.array(traj)\n",
        "\n",
        "    # Smooth x and y separately\n",
        "    x_smooth = savgol_filter(traj_np[:, 0], window_size, poly_order)\n",
        "    y_smooth = savgol_filter(traj_np[:, 1], window_size, poly_order)\n",
        "\n",
        "    # Combine back to pairs\n",
        "    smoothed_traj = list(zip(x_smooth, y_smooth))\n",
        "\n",
        "    return smoothed_traj\n",
        "\n",
        "# Step 8-10: Enhanced tracking with occlusion handling and prediction\n",
        "def track_with_occlusion_handling(video_path, model, max_frames=50, output_path=None):\n",
        "    \"\"\"Advanced tracking function with occlusion handling and prediction\"\"\"\n",
        "\n",
        "    # Create optimized tracker\n",
        "    tracker = create_optimized_tracker()\n",
        "\n",
        "    # Dictionary to store trajectories for each track ID\n",
        "    trajectories = defaultdict(list)\n",
        "\n",
        "    # Dictionary to store smoothed trajectories\n",
        "    smooth_trajectories = defaultdict(list)\n",
        "\n",
        "    # Dictionary to store Kalman filters for each track\n",
        "    kalman_filters = {}\n",
        "\n",
        "    # Dictionary to store visibility status (True = visible, False = occluded)\n",
        "    is_visible = defaultdict(bool)\n",
        "\n",
        "    # Dictionary to store occlusion count and duration\n",
        "    occlusion_stats = defaultdict(lambda: {\"count\": 0, \"total_frames\": 0, \"current_streak\": 0})\n",
        "\n",
        "    # Dictionary to store speed for each track ID\n",
        "    speeds = defaultdict(float)\n",
        "\n",
        "    # Dictionary to store behavior for each track ID\n",
        "    behaviors = defaultdict(str)\n",
        "\n",
        "    # Dictionary to store violations for each track ID\n",
        "    violations = defaultdict(bool)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(f\"Video FPS: {fps}, Resolution: {frame_width}x{frame_height}\")\n",
        "\n",
        "    # Define intersection zones\n",
        "    intersection = define_intersection_zones(frame_width, frame_height)\n",
        "\n",
        "    # Set up video writer if output path is provided\n",
        "    video_writer = None\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        video_writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    # Variables for tracking statistics\n",
        "    frame_count = 0\n",
        "    total_vehicles = 0\n",
        "    total_unique_vehicles = 0\n",
        "    total_violations = 0\n",
        "    total_occlusions = 0\n",
        "    processing_times = []\n",
        "\n",
        "    # For visualization\n",
        "    colors = np.random.randint(0, 255, size=(100, 3), dtype=np.uint8)\n",
        "\n",
        "    # Process video frames\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"End of video.\")\n",
        "            break\n",
        "\n",
        "        # Start timing for this frame processing\n",
        "        frame_start_time = time.time()\n",
        "\n",
        "        # Update traffic light states\n",
        "        intersection = update_traffic_lights(intersection, frame_count)\n",
        "\n",
        "        # Perform detection inference\n",
        "        results = model(frame)\n",
        "\n",
        "        # Get detections as numpy array [x1, y1, x2, y2, confidence, class]\n",
        "        detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "        # Format detections for DeepSORT [x1, y1, width, height, confidence, class]\n",
        "        detection_list = []\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            detection_list.append(\n",
        "                ([x1, y1, x2 - x1, y2 - y1], conf, int(cls))\n",
        "            )\n",
        "\n",
        "        # Update tracker with detections\n",
        "        tracks = tracker.update_tracks(detection_list, frame=frame)\n",
        "\n",
        "        # Count active tracks in this frame\n",
        "        active_tracks = [track for track in tracks if track.is_confirmed()]\n",
        "        frame_vehicle_count = len(active_tracks)\n",
        "        total_vehicles += frame_vehicle_count\n",
        "\n",
        "        # Draw tracking results on the frame\n",
        "        tracked_frame = frame.copy()\n",
        "\n",
        "        # Draw intersection zones\n",
        "        tracked_frame = draw_intersection_zones(tracked_frame, intersection)\n",
        "\n",
        "        # Update visibility status for all tracks\n",
        "        current_track_ids = set()\n",
        "\n",
        "        # Process each active track\n",
        "        for track in active_tracks:\n",
        "            track_id = track.track_id\n",
        "            current_track_ids.add(track_id)\n",
        "\n",
        "            # Mark track as visible\n",
        "            is_visible[track_id] = True\n",
        "\n",
        "            # Reset occlusion streak counter\n",
        "            occlusion_stats[track_id][\"current_streak\"] = 0\n",
        "\n",
        "            bbox = track.to_ltrb()\n",
        "            x1, y1, x2, y2 = bbox\n",
        "\n",
        "            # Get track position (center of bounding box)\n",
        "            center_x = int((x1 + x2) / 2)\n",
        "            center_y = int((y1 + y2) / 2)\n",
        "            current_pos = (center_x, center_y)\n",
        "\n",
        "            # Create Kalman filter for new tracks\n",
        "            if track_id not in kalman_filters:\n",
        "                kalman_filters[track_id] = VehicleKalmanFilter(current_pos, dt=1.0/fps)\n",
        "\n",
        "            # Update Kalman filter with new position\n",
        "            kalman_pos = kalman_filters[track_id].update(current_pos)\n",
        "\n",
        "            # Add point to trajectory\n",
        "            trajectories[track_id].append(current_pos)\n",
        "\n",
        "            # Update smoothed trajectory\n",
        "            if len(trajectories[track_id]) > 5:\n",
        "                smooth_trajectories[track_id] = smooth_trajectory(trajectories[track_id])\n",
        "            else:\n",
        "                smooth_trajectories[track_id] = trajectories[track_id].copy()\n",
        "\n",
        "            # Update speed estimate\n",
        "            speeds[track_id] = estimate_speed(trajectories[track_id], fps)\n",
        "\n",
        "            # Count unique vehicles\n",
        "            if len(trajectories[track_id]) == 1:\n",
        "                total_unique_vehicles += 1\n",
        "\n",
        "            # Continue with behavior prediction and visualization...\n",
        "\n",
        "        # Step 9: Handle occlusions - check for tracks that are not visible in this frame but were active recently\n",
        "        all_known_tracks = set(trajectories.keys())\n",
        "        occluded_tracks = all_known_tracks - current_track_ids\n",
        "\n",
        "        for track_id in occluded_tracks:\n",
        "            # Skip if track has been missing for too long (more than max_age frames)\n",
        "            if frame_count - len(trajectories[track_id]) > tracker.tracker.max_age:\n",
        "                continue\n",
        "\n",
        "            # Track is occluded in this frame\n",
        "            if is_visible[track_id]:  # First frame of occlusion\n",
        "                is_visible[track_id] = False\n",
        "                occlusion_stats[track_id][\"count\"] += 1\n",
        "                total_occlusions += 1\n",
        "\n",
        "            # Update occlusion streak counter\n",
        "            occlusion_stats[track_id][\"current_streak\"] += 1\n",
        "            occlusion_stats[track_id][\"total_frames\"] += 1\n",
        "\n",
        "            # Use Kalman filter to predict position during occlusion\n",
        "            if track_id in kalman_filters and len(trajectories[track_id]) > 0:\n",
        "                # Predict position\n",
        "                predicted_pos = kalman_filters[track_id].predict(steps=1)\n",
        "\n",
        "                # Add predicted position to trajectory (marked differently)\n",
        "                trajectories[track_id].append(predicted_pos)\n",
        "\n",
        "                # Update smoothed trajectory\n",
        "                smooth_trajectories[track_id] = smooth_trajectory(trajectories[track_id])\n",
        "\n",
        "                # Draw occluded bounding box and trajectory with dashed lines\n",
        "                last_width = last_height = 50  # Default size\n",
        "                if len(trajectories[track_id]) > 1:\n",
        "                    # Estimate object size from last known position\n",
        "                    last_point = trajectories[track_id][-2]\n",
        "                    predicted_point = predicted_pos\n",
        "                    # Draw predicted bounding box (dashed)\n",
        "                    cv2.rectangle(tracked_frame,\n",
        "                                  (int(predicted_point[0] - last_width/2), int(predicted_point[1] - last_height/2)),\n",
        "                                  (int(predicted_point[0] + last_width/2), int(predicted_point[1] + last_height/2)),\n",
        "                                  (255, 255, 0), 1, cv2.LINE_DASHED)\n",
        "\n",
        "                    # Draw text\n",
        "                    cv2.putText(tracked_frame, f\"Occluded ID:{track_id}\",\n",
        "                                (int(predicted_point[0] - last_width/2), int(predicted_point[1] - last_height/2 - 10)),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
        "\n",
        "        # Draw all active tracks (both visible and predicted)\n",
        "        for track_id in current_track_ids:\n",
        "            # Get color for this track ID\n",
        "            color = colors[track_id % len(colors)].tolist()\n",
        "\n",
        "            # Get current bounding box for visible tracks\n",
        "            for track in active_tracks:\n",
        "                if track.track_id == track_id:\n",
        "                    bbox = track.to_ltrb()\n",
        "                    x1, y1, x2, y2 = bbox\n",
        "\n",
        "                    # Determine which region of the intersection the vehicle is in\n",
        "                    center_x = int((x1 + x2) / 2)\n",
        "                    center_y = int((y1 + y2) / 2)\n",
        "\n",
        "                    if center_x < intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                        approach_direction = 'north'\n",
        "                    elif center_x >= intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                        approach_direction = 'east'\n",
        "                    elif center_x < intersection['center'][0] and center_y >= intersection['center'][1]:\n",
        "                        approach_direction = 'west'\n",
        "                    else:\n",
        "                        approach_direction = 'south'\n",
        "\n",
        "                    # Step 3: Analyze behavior if we have enough trajectory history\n",
        "                    if len(trajectories[track_id]) >= 10:\n",
        "                        behaviors[track_id] = analyze_trajectory_direction(trajectories[track_id])\n",
        "\n",
        "                        # Step 4: Check for red light violation\n",
        "                        stop_line_y = intersection['stop_lines'][approach_direction]\n",
        "                        light_state = intersection['light_states'][approach_direction]\n",
        "\n",
        "                        # Detect violation\n",
        "                        new_violation = detect_red_light_violation(\n",
        "                            trajectories[track_id],\n",
        "                            (center_x, center_y),\n",
        "                            stop_line_y,\n",
        "                            light_state\n",
        "                        )\n",
        "\n",
        "                        # Count new violations\n",
        "                        if new_violation and not violations[track_id]:\n",
        "                            total_violations += 1\n",
        "\n",
        "                        violations[track_id] = new_violation\n",
        "\n",
        "                    # Draw bounding box (red for violation, normal color otherwise)\n",
        "                    box_color = (0, 0, 255) if violations[track_id] else color\n",
        "                    cv2.rectangle(tracked_frame, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)\n",
        "\n",
        "                    # Add ID and behavior text\n",
        "                    behavior_text = f\"ID:{track_id} {behaviors[track_id]}\"\n",
        "                    cv2.putText(tracked_frame, behavior_text, (int(x1), int(y1)-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "                    # Add speed text\n",
        "                    speed_text = f\"Speed:{speeds[track_id]:.1f}\"\n",
        "                    cv2.putText(tracked_frame, speed_text, (int(x1), int(y1)-30),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "                    # Add violation warning\n",
        "                    if violations[track_id]:\n",
        "                        cv2.putText(tracked_frame, \"VIOLATION!\", (int(x1), int(y1)-50),\n",
        "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "            # Draw trajectory\n",
        "            if len(smooth_trajectories[track_id]) > 1:\n",
        "                # Draw the smoothed trajectory line\n",
        "                for i in range(1, len(smooth_trajectories[track_id])):\n",
        "                    pt1 = tuple(map(int, smooth_trajectories[track_id][i-1]))\n",
        "                    pt2 = tuple(map(int, smooth_trajectories[track_id][i]))\n",
        "\n",
        "                    # Draw predicted portions with dashed line if occluded\n",
        "                    if i >= len(trajectories[track_id]) - occlusion_stats[track_id][\"current_streak\"]:\n",
        "                        # Dashed line for predicted positions\n",
        "                        for j in range(0, 10, 2):  # Draw dashed line\n",
        "                            sub_pt1 = (int(pt1[0] + (pt2[0] - pt1[0]) * j / 10),\n",
        "                                      int(pt1[1] + (pt2[1] - pt1[1]) * j / 10))\n",
        "                            sub_pt2 = (int(pt1[0] + (pt2[0] - pt1[0]) * (j + 1) / 10),\n",
        "                                      int(pt1[1] + (pt2[1] - pt1[1]) * (j + 1) / 10))\n",
        "                            cv2.line(tracked_frame, sub_pt1, sub_pt2, (255, 255, 0), 2)\n",
        "                    else:\n",
        "                        # Solid line for actual positions\n",
        "                        cv2.line(tracked_frame, pt1, pt2, color, 2)\n",
        "\n",
        "        # Step 10: Add handling for congested scenes (draw density map)\n",
        "        if frame_vehicle_count > 5:  # Only show heatmap in congested scenes\n",
        "            # Create a density heatmap of vehicle positions\n",
        "            heatmap = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "\n",
        "            for track_id in current_track_ids:\n",
        "                if len(trajectories[track_id]) > 0:\n",
        "                    # Get last position\n",
        "                    pos = trajectories[track_id][-1]\n",
        "                    x, y = int(pos[0]), int(pos[1])\n",
        "\n",
        "                    # Draw a gaussian blob at vehicle position\n",
        "                    cv2.circle(heatmap, (x, y), 30, 255, -1)\n",
        "\n",
        "            # Blur the heatmap to create density effect\n",
        "            heatmap = cv2.GaussianBlur(heatmap, (99, 99), 0)\n",
        "\n",
        "            # Normalize and colorize heatmap\n",
        "            heatmap_normalized = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)\n",
        "            heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)\n",
        "\n",
        "            # Blend with original frame\n",
        "            alpha = 0.3  # Transparency factor\n",
        "            density_overlay = cv2.addWeighted(tracked_frame, 1 - alpha, heatmap_colored, alpha, 0)\n",
        "\n",
        "            # Add density info text\n",
        "            cv2.putText(density_overlay, \"Traffic Density Map\", (10, 120),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "            tracked_frame = density_overlay\n",
        "\n",
        "        # Calculate processing time and FPS\n",
        "        processing_time = time.time() - frame_start_time\n",
        "        processing_times.append(processing_time)\n",
        "        fps_current = 1.0 / processing_time if processing_time > 0 else 0\n",
        "\n",
        "        # Add FPS text to the frame\n",
        "        cv2.putText(tracked_frame, f\"FPS: {fps_current:.2f}\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add detection and tracking info\n",
        "        cv2.putText(tracked_frame, f\"Tracking: {frame_vehicle_count} vehicles\", (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add violation counter\n",
        "        cv2.putText(tracked_frame, f\"Violations: {total_violations}\", (10, 90),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Add occlusion counter\n",
        "        cv2.putText(tracked_frame, f\"Occlusions: {total_occlusions}\", (10, 150),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
        "\n",
        "        # Write frame to output video if requested\n",
        "        if video_writer:\n",
        "            video_writer.write(tracked_frame)\n",
        "\n",
        "        # Convert BGR to RGB for matplotlib display\n",
        "        tracked_frame_rgb = cv2.cvtColor(tracked_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the frame\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(tracked_frame_rgb)\n",
        "        plt.title(f\"Frame {frame_count}: {frame_vehicle_count} vehicles tracked | FPS: {fps_current:.2f}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # For demo purposes, process a limited number of frames\n",
        "        if frame_count >= max_frames:  # Increase this to process more frames\n",
        "            break\n",
        "\n",
        "    # Release video resources\n",
        "    cap.release()\n",
        "    if video_writer:\n",
        "        video_writer.release()\n",
        "\n",
        "    # Calculate and display performance metrics\n",
        "    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0\n",
        "    avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "    print(f\"Processed {frame_count} frames\")\n",
        "    print(f\"Average processing time: {avg_processing_time:.3f} seconds per frame\")\n",
        "    print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "    print(f\"Total unique vehicles tracked: {total_unique_vehicles}\")\n",
        "    print(f\"Average vehicles per frame: {total_vehicles / frame_count:.2f}\")\n",
        "    print(f\"Total red light violations detected: {total_violations}\")\n",
        "    print(f\"Total occlusion events detected: {total_occlusions}\")\n",
        "\n",
        "    # Calculate occlusion statistics\n",
        "    if occlusion_stats:\n",
        "        total_occlusion_frames = sum(stats[\"total_frames\"] for stats in occlusion_stats.values())\n",
        "        avg_occlusion_duration = total_occlusion_frames / total_occlusions if total_occlusions > 0 else 0\n",
        "        print(f\"Average occlusion duration: {avg_occlusion_duration:.2f} frames\")\n",
        "\n",
        "        # Track recovery statistics\n",
        "        recovered_tracks = sum(1 for track_id, stats in occlusion_stats.items()\n",
        "                              if stats[\"count\"] > 0 and is_visible[track_id])\n",
        "        recovery_rate = recovered_tracks / total_occlusions if total_occlusions > 0 else 0\n",
        "        print(f\"Track recovery rate: {recovery_rate * 100:.1f}% ({recovered_tracks}/{total_occlusions})\")\n",
        "\n",
        "    # Return data for further analysis\n",
        "    return {\n",
        "        'trajectories': trajectories,\n",
        "        'smooth_trajectories': smooth_trajectories,\n",
        "        'behaviors': behaviors,\n",
        "        'violations': violations,\n",
        "        'speeds': speeds,\n",
        "        'occlusion_stats': occlusion_stats,\n",
        "        'metrics': {\n",
        "            'total_frames': frame_count,\n",
        "            'total_unique_vehicles': total_unique_vehicles,\n",
        "            'total_violations': total_violations,\n",
        "            'total_occlusions': total_occlusions,\n",
        "            'avg_fps': avg_fps\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Functions from Milestone 3 needed for the implementation\n",
        "\n",
        "def analyze_trajectory_direction(trajectory, window_size=10):\n",
        "    \"\"\"Analyze trajectory to determine if vehicle is turning and in which direction\"\"\"\n",
        "    if len(trajectory) < window_size:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    # Get recent trajectory points\n",
        "    recent_traj = trajectory[-window_size:]\n",
        "\n",
        "    # Calculate direction changes\n",
        "    angles = []\n",
        "    for i in range(1, len(recent_traj)):\n",
        "        dx = recent_traj[i][0] - recent_traj[i-1][0]\n",
        "        dy = recent_traj[i][1] - recent_traj[i-1][1]\n",
        "        angle = math.atan2(dy, dx)\n",
        "        angles.append(angle)\n",
        "\n",
        "    # Calculate angle difference between start and end\n",
        "    angle_diff = abs(angles[-1] - angles[0])\n",
        "\n",
        "    # Classify turn direction\n",
        "    if angle_diff > 0.3:  # Threshold for turn detection (in radians)\n",
        "        # Determine turn direction\n",
        "        if (angles[-1] - angles[0]) > 0:\n",
        "            return \"Turning Left\"\n",
        "        else:\n",
        "            return \"Turning Right\"\n",
        "    else:\n",
        "        return \"Going Straight\"\n",
        "\n",
        "def detect_red_light_violation(trajectory, current_pos, stop_line_y, light_state):\n",
        "    \"\"\"Detect if a vehicle might run a red light\"\"\"\n",
        "    if light_state != \"RED\" or len(trajectory) < 5:\n",
        "        return False\n",
        "\n",
        "    # Check if vehicle is approaching the stop line\n",
        "    if current_pos[1] < stop_line_y and current_pos[1] > stop_line_y - 100:\n",
        "        # Calculate speed\n",
        "        recent_traj = trajectory[-5:]\n",
        "        distances = []\n",
        "        for i in range(1, len(recent_traj)):\n",
        "            distances.append(calculate_distance(recent_traj[i], recent_traj[i-1]))\n",
        "\n",
        "        avg_speed = sum(distances) / len(distances)\n",
        "\n",
        "        # If approaching fast and not slowing down\n",
        "        if avg_speed > 5.0:  # Threshold for \"fast\" approach\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def define_intersection_zones(frame_width, frame_height):\n",
        "    \"\"\"Define virtual zones for traffic analysis based on frame dimensions\"\"\"\n",
        "    # Example for a four-way intersection\n",
        "    intersection = {\n",
        "        'center': (frame_width//2, frame_height//2),\n",
        "        'radius': 150,\n",
        "        'stop_lines': {\n",
        "            'north': frame_height//2 - 100,\n",
        "            'south': frame_height//2 + 100,\n",
        "            'east': frame_width//2 + 100,\n",
        "            'west': frame_width//2 - 100\n",
        "        },\n",
        "        'light_states': {\n",
        "            'north': 'RED',\n",
        "            'south': 'RED',\n",
        "            'east': 'GREEN',\n",
        "            'west': 'RED'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return intersection\n",
        "\n",
        "def draw_intersection_zones(frame, intersection):\n",
        "    \"\"\"Draw intersection zones on the frame\"\"\"\n",
        "    frame_width = frame.shape[1]\n",
        "    frame_height = frame.shape[0]\n",
        "\n",
        "    # Draw center circle\n",
        "    cv2.circle(frame, intersection['center'], 10, (0, 0, 255), -1)\n",
        "    cv2.circle(frame, intersection['center'], intersection['radius'], (0, 0, 255), 2)\n",
        "\n",
        "    # Draw stop lines\n",
        "    cv2.line(frame,\n",
        "             (0, intersection['stop_lines']['north']),\n",
        "             (frame_width, intersection['stop_lines']['north']),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (0, intersection['stop_lines']['south']),\n",
        "             (frame_width, intersection['stop_lines']['south']),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (intersection['stop_lines']['west'], 0),\n",
        "             (intersection['stop_lines']['west'], frame_height),\n",
        "             (255, 0, 0), 2)\n",
        "    cv2.line(frame,\n",
        "             (intersection['stop_lines']['east'], 0),\n",
        "             (intersection['stop_lines']['east'], frame_height),\n",
        "             (255, 0, 0), 2)\n",
        "\n",
        "    # Add traffic light indicators\n",
        "    light_positions = {\n",
        "        'north': (frame_width//2 - 50, intersection['stop_lines']['north'] - 20),\n",
        "        'south': (frame_width//2 + 50, intersection['stop_lines']['south'] + 20),\n",
        "        'east': (intersection['stop_lines']['east'] + 20, frame_height//2 - 50),\n",
        "        'west': (intersection['stop_lines']['west'] - 20, frame_height//2 + 50)\n",
        "    }\n",
        "\n",
        "    for direction, position in light_positions.items():\n",
        "        # Determine light color\n",
        "        if intersection['light_states'][direction] == 'RED':\n",
        "            light_color = (0, 0, 255)  # Red\n",
        "        elif intersection['light_states'][direction] == 'YELLOW':\n",
        "            light_color = (0, 255, 255)  # Yellow\n",
        "        else:  # GREEN\n",
        "            light_color = (0, 255, 0)  # Green\n",
        "\n",
        "        # Draw light indicator\n",
        "        cv2.circle(frame, position, 15, light_color, -1)\n",
        "        cv2.circle(frame, position, 15, (255, 255, 255), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "def update_traffic_lights(intersection, frame_count, cycle_length=150):\n",
        "    \"\"\"Update traffic light states based on a simple cycle\"\"\"\n",
        "    # Simple traffic light cycle: N/S green, E/W red, then switch\n",
        "    cycle_position = frame_count % cycle_length\n",
        "\n",
        "    # First half of cycle: N/S green, E/W red\n",
        "    if cycle_position < cycle_length // 2:\n",
        "        intersection['light_states']['north'] = 'GREEN'\n",
        "        intersection['light_states']['south'] = 'GREEN'\n",
        "        intersection['light_states']['east'] = 'RED'\n",
        "        intersection['light_states']['west'] = 'RED'\n",
        "\n",
        "        # Add yellow transition\n",
        "        if cycle_position > (cycle_length // 2) - 30:\n",
        "            intersection['light_states']['north'] = 'YELLOW'\n",
        "            intersection['light_states']['south'] = 'YELLOW'\n",
        "    # Second half: N/S red, E/W green\n",
        "    else:\n",
        "        intersection['light_states']['north'] = 'RED'\n",
        "        intersection['light_states']['south'] = 'RED'\n",
        "        intersection['light_states']['east'] = 'GREEN'\n",
        "        intersection['light_states']['west'] = 'GREEN'\n",
        "\n",
        "        # Add yellow transition\n",
        "        if cycle_position > cycle_length - 30:\n",
        "            intersection['light_states']['east'] = 'YELLOW'\n",
        "            intersection['light_states']['west'] = 'YELLOW'\n",
        "\n",
        "    return intersection\n",
        "\n",
        "# Function to visualize occlusion handling results\n",
        "def visualize_occlusion_results(tracking_results, frame_width, frame_height):\n",
        "    \"\"\"Create visualizations to analyze occlusion handling effectiveness\"\"\"\n",
        "\n",
        "    trajectories = tracking_results['trajectories']\n",
        "    smooth_trajectories = tracking_results['smooth_trajectories']\n",
        "    occlusion_stats = tracking_results['occlusion_stats']\n",
        "\n",
        "    # 1. Track continuity visualization - showing original and smoothed trajectories\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    plt.title(\"Track Continuity: Original vs Smoothed Trajectories\")\n",
        "\n",
        "    # Draw intersection for reference\n",
        "    intersection = define_intersection_zones(frame_width, frame_height)\n",
        "    plt.plot(intersection['center'][0], intersection['center'][1], 'ko', markersize=10)\n",
        "    plt.plot([0, frame_width], [intersection['stop_lines']['north'], intersection['stop_lines']['north']], 'k--')\n",
        "    plt.plot([0, frame_width], [intersection['stop_lines']['south'], intersection['stop_lines']['south']], 'k--')\n",
        "    plt.plot([intersection['stop_lines']['west'], intersection['stop_lines']['west']], [0, frame_height], 'k--')\n",
        "    plt.plot([intersection['stop_lines']['east'], intersection['stop_lines']['east']], [0, frame_height], 'k--')\n",
        "\n",
        "    # Select tracks that had occlusions for visualization\n",
        "    occluded_track_ids = [track_id for track_id, stats in occlusion_stats.items()\n",
        "                         if stats[\"count\"] > 0 and len(trajectories[track_id]) > 10]\n",
        "\n",
        "    # Limit to 5 tracks to avoid clutter\n",
        "    selected_tracks = occluded_track_ids[:5] if len(occluded_track_ids) > 5 else occluded_track_ids\n",
        "\n",
        "    for track_id in selected_tracks:\n",
        "        # Get original trajectory\n",
        "        orig_traj = np.array(trajectories[track_id])\n",
        "\n",
        "        # Get smoothed trajectory\n",
        "        smooth_traj = np.array(smooth_trajectories[track_id] if track_id in smooth_trajectories\n",
        "                              else trajectories[track_id])\n",
        "\n",
        "        # Plot original trajectory as dots\n",
        "        plt.plot(orig_traj[:, 0], orig_traj[:, 1], '.', markersize=4, label=f\"Original ID:{track_id}\")\n",
        "\n",
        "        # Plot smoothed trajectory as line\n",
        "        plt.plot(smooth_traj[:, 0], smooth_traj[:, 1], '-', linewidth=2, label=f\"Smoothed ID:{track_id}\")\n",
        "\n",
        "    plt.xlim(0, frame_width)\n",
        "    plt.ylim(frame_height, 0)  # Invert y-axis to match image coordinates\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Occlusion statistics visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Filter to tracks with occlusions\n",
        "    occlusion_counts = {track_id: stats[\"count\"] for track_id, stats in occlusion_stats.items()\n",
        "                       if stats[\"count\"] > 0}\n",
        "\n",
        "    # Sort by occlusion count\n",
        "    sorted_occlusions = sorted(occlusion_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get top 15 tracks with most occlusions\n",
        "    top_occluded = sorted_occlusions[:15]\n",
        "\n",
        "    # Create bar chart\n",
        "    track_ids = [f\"ID:{tid}\" for tid, _ in top_occluded]\n",
        "    occlusion_counts = [count for _, count in top_occluded]\n",
        "\n",
        "    plt.bar(track_ids, occlusion_counts, color='orange')\n",
        "    plt.title(\"Occlusion Counts by Vehicle Track\")\n",
        "    plt.xlabel(\"Track ID\")\n",
        "    plt.ylabel(\"Number of Occlusion Events\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Occlusion duration histogram\n",
        "    occlusion_durations = [stats[\"total_frames\"] / stats[\"count\"] if stats[\"count\"] > 0 else 0\n",
        "                          for stats in occlusion_stats.values()]\n",
        "    occlusion_durations = [d for d in occlusion_durations if d > 0]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(occlusion_durations, bins=20, color='green', alpha=0.7)\n",
        "    plt.title(\"Distribution of Occlusion Durations\")\n",
        "    plt.xlabel(\"Average Occlusion Duration (frames)\")\n",
        "    plt.ylabel(\"Number of Tracks\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test the implementation\n",
        "def test_occlusion_handling(model, video_path, max_frames=30):\n",
        "    \"\"\"Test and evaluate occlusion handling\"\"\"\n",
        "\n",
        "    # Make sure we have necessary imports and functions from previous implementation\n",
        "    print(\"Starting occlusion handling test...\")\n",
        "\n",
        "    # Run tracking with occlusion handling\n",
        "    results = track_with_occlusion_handling(\n",
        "        video_path=video_path,\n",
        "        model=model,\n",
        "        max_frames=max_frames,\n",
        "        output_path=\"occlusion_handling_output.mp4\"\n",
        "    )\n",
        "\n",
        "    # Get video dimensions for visualization\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "\n",
        "    # Analyze and visualize results\n",
        "    visualize_occlusion_results(results, width, height)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "PzimZ-Skowqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Occlusion Evaluation Tests\n",
        "# This script tests our tracker's performance in handling occlusions and complex scenarios\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Import our implementation from Milestone 4\n",
        "# (This assumes all the necessary functions from the milestone4-implementation.py file)\n",
        "\n",
        "# Function to create synthetic occlusion scenarios for controlled testing\n",
        "def create_synthetic_occlusion_test(base_video_path, output_path, occlusion_frames=[30, 60, 90],\n",
        "                                   occlusion_size=0.3):\n",
        "    \"\"\"\n",
        "    Create a test video with synthetic occlusions (black boxes) to evaluate tracking robustness\n",
        "\n",
        "    Args:\n",
        "        base_video_path: Path to the original video\n",
        "        output_path: Path to save the modified video\n",
        "        occlusion_frames: List of frame numbers where to introduce occlusions\n",
        "        occlusion_size: Size of occlusion as a fraction of the frame size\n",
        "    \"\"\"\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(base_video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Define occlusion size\n",
        "    occlusion_width = int(width * occlusion_size)\n",
        "    occlusion_height = int(height * occlusion_size)\n",
        "\n",
        "    # Define occlusion positions (center of frame for simplicity)\n",
        "    occlusion_x = width // 2 - occlusion_width // 2\n",
        "    occlusion_y = height // 2 - occlusion_height // 2\n",
        "\n",
        "    # Create output video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_number = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Check if we should add occlusion to this frame\n",
        "        if frame_number in occlusion_frames:\n",
        "            # Create a black occlusion rectangle\n",
        "            frame[occlusion_y:occlusion_y+occlusion_height,\n",
        "                 occlusion_x:occlusion_x+occlusion_width] = 0\n",
        "\n",
        "            # Add occlusion indicator text\n",
        "            cv2.putText(frame, \"SYNTHETIC OCCLUSION\", (occlusion_x, occlusion_y-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "        # Write the frame\n",
        "        out.write(frame)\n",
        "        frame_number += 1\n",
        "\n",
        "        # For demo purposes only process first 120 frames\n",
        "        if frame_number >= 120:\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"Created synthetic occlusion test video at {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Compare tracking performance with and without occlusion handling\n",
        "def compare_tracking_performance(model, video_path, max_frames=30):\n",
        "    \"\"\"\n",
        "    Compare tracking performance with regular tracking vs. occlusion-optimized tracking\n",
        "\n",
        "    Args:\n",
        "        model: The detection model to use\n",
        "        video_path: Path to the video for testing\n",
        "        max_frames: Number of frames to process\n",
        "    \"\"\"\n",
        "    print(\"Comparing tracking methods...\")\n",
        "\n",
        "    # Track using standard DeepSORT (from Milestone 3)\n",
        "    # This function should be imported from your Milestone 3 implementation\n",
        "    standard_results = track_and_predict_behavior(\n",
        "        video_path,\n",
        "        model,\n",
        "        max_frames=max_frames\n",
        "    )\n",
        "\n",
        "    # Track using occlusion-optimized tracking (from Milestone 4)\n",
        "    occlusion_results = track_with_occlusion_handling(\n",
        "        video_path,\n",
        "        model,\n",
        "        max_frames=max_frames\n",
        "    )\n",
        "\n",
        "    # Compare results\n",
        "    standard_tracks = len(standard_results['trajectories'])\n",
        "    occlusion_tracks = len(occlusion_results['trajectories'])\n",
        "\n",
        "    # Calculate average track length\n",
        "    standard_avg_len = sum(len(traj) for traj in standard_results['trajectories'].values()) / standard_tracks\n",
        "    occlusion_avg_len = sum(len(traj) for traj in occlusion_results['trajectories'].values()) / occlusion_tracks\n",
        "\n",
        "    print(\"\\nTracking Performance Comparison:\")\n",
        "    print(f\"Standard Tracking: {standard_tracks} tracks, avg length {standard_avg_len:.2f} frames\")\n",
        "    print(f\"Occlusion-optimized: {occlusion_tracks} tracks, avg length {occlusion_avg_len:.2f} frames\")\n",
        "\n",
        "    # Calculate improvement percentage\n",
        "    len_improvement = (occlusion_avg_len - standard_avg_len) / standard_avg_len * 100\n",
        "    print(f\"Track length improvement: {len_improvement:.2f}%\")\n",
        "\n",
        "    # Visualize track length comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Get track lengths\n",
        "    standard_lengths = [len(traj) for traj in standard_results['trajectories'].values()]\n",
        "    occlusion_lengths = [len(traj) for traj in occlusion_results['trajectories'].values()]\n",
        "\n",
        "    # Create histograms\n",
        "    bins = range(0, max(max(standard_lengths), max(occlusion_lengths)) + 5, 5)\n",
        "    plt.hist(standard_lengths, bins=bins, alpha=0.5, label='Standard Tracking')\n",
        "    plt.hist(occlusion_lengths, bins=bins, alpha=0.5, label='Occlusion-optimized')\n",
        "\n",
        "    plt.title('Track Length Distribution Comparison')\n",
        "    plt.xlabel('Track Length (frames)')\n",
        "    plt.ylabel('Number of Tracks')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'standard': standard_results,\n",
        "        'occlusion': occlusion_results\n",
        "    }\n",
        "\n",
        "# Function to measure track fragmentation (ID switches)\n",
        "def analyze_track_fragmentation(results, ground_truth=None):\n",
        "    \"\"\"\n",
        "    Analyze how much track fragmentation occurs (when one physical object gets multiple IDs)\n",
        "\n",
        "    Args:\n",
        "        results: Tracking results\n",
        "        ground_truth: Optional ground truth data (if available)\n",
        "    \"\"\"\n",
        "    trajectories = results['trajectories']\n",
        "\n",
        "    # If no ground truth, we'll use spatial-temporal overlap analysis\n",
        "    # to estimate potential track fragments\n",
        "\n",
        "    # Find tracks that could be fragments of the same physical object\n",
        "    potential_fragments = []\n",
        "\n",
        "    # Get all track IDs\n",
        "    track_ids = list(trajectories.keys())\n",
        "\n",
        "    for i, track_id1 in enumerate(track_ids):\n",
        "        fragments = [track_id1]\n",
        "        traj1 = trajectories[track_id1]\n",
        "\n",
        "        if len(traj1) < 3:  # Skip very short tracks\n",
        "            continue\n",
        "\n",
        "        # Get end time and position of track1\n",
        "        end_time1 = len(traj1)\n",
        "        end_pos1 = traj1[-1]\n",
        "\n",
        "        for j in range(i+1, len(track_ids)):\n",
        "            track_id2 = track_ids[j]\n",
        "            traj2 = trajectories[track_id2]\n",
        "\n",
        "            if len(traj2) < 3:  # Skip very short tracks\n",
        "                continue\n",
        "\n",
        "            # Get start time and position of track2\n",
        "            start_pos2 = traj2[0]\n",
        "\n",
        "            # Check if track2 starts after track1 ends\n",
        "            # and check if start position of track2 is close to end position of track1\n",
        "            distance = np.sqrt((end_pos1[0] - start_pos2[0])**2 + (end_pos1[1] - start_pos2[1])**2)\n",
        "\n",
        "            # If tracks are close in space and time, they might be fragments\n",
        "            max_spatial_gap = 50  # pixels\n",
        "            max_temporal_gap = 10  # frames\n",
        "\n",
        "            if distance < max_spatial_gap:\n",
        "                fragments.append(track_id2)\n",
        "\n",
        "        if len(fragments) > 1:\n",
        "            potential_fragments.append(fragments)\n",
        "\n",
        "    # Print fragmentation analysis\n",
        "    print(f\"\\nTrack Fragmentation Analysis:\")\n",
        "    print(f\"Total tracks: {len(trajectories)}\")\n",
        "    print(f\"Potential track fragments: {len(potential_fragments)} groups\")\n",
        "\n",
        "    if potential_fragments:\n",
        "        print(\"Fragment groups:\")\n",
        "        for i, group in enumerate(potential_fragments):\n",
        "            print(f\"  Group {i+1}: Track IDs {group}\")\n",
        "\n",
        "    return potential_fragments\n",
        "\n",
        "# Execution function\n",
        "def execute_occlusion_tests(model, base_video_path):\n",
        "    \"\"\"Execute the occlusion evaluation tests\"\"\"\n",
        "\n",
        "    # 1. Create synthetic occlusion test video\n",
        "    synthetic_test_path = create_synthetic_occlusion_test(\n",
        "        base_video_path,\n",
        "        \"synthetic_occlusion_test.mp4\"\n",
        "    )\n",
        "\n",
        "    # 2. Compare tracking methods on the synthetic test\n",
        "    comparison_results = compare_tracking_performance(\n",
        "        model,\n",
        "        synthetic_test_path,\n",
        "        max_frames=120\n",
        "    )\n",
        "\n",
        "    # 3. Analyze track fragmentation\n",
        "    fragmentation_analysis = analyze_track_fragmentation(comparison_results['occlusion'])\n",
        "\n",
        "    # 4. Test on a challenging real video section (if available)\n",
        "    challenging_section_path = base_video_path  # Use the same video for simplicity\n",
        "\n",
        "    print(\"\\nTesting on challenging video section...\")\n",
        "    challenging_results = track_with_occlusion_handling(\n",
        "        challenging_section_path,\n",
        "        model,\n",
        "        max_frames=30\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'synthetic_test': synthetic_test_path,\n",
        "        'comparison': comparison_results,\n",
        "        'fragmentation': fragmentation_analysis,\n",
        "        'challenging': challenging_results\n",
        "    }\n",
        "\n",
        "# Main execution function\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "\n",
        "    # Video path\n",
        "    video_path = '/kaggle/input/videotraffic/Traffic.mp4'\n",
        "\n",
        "    # Run tests\n",
        "    results = execute_occlusion_tests(model, video_path)\n",
        "\n",
        "    print(\"\\nOcclusion testing complete!\")"
      ],
      "metadata": {
        "id": "Nq-dyhLppJuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the Milestone 4 implementation\n",
        "\n",
        "# First, let's install the required packages if not already installed\n",
        "!pip install -q deep-sort-realtime filterpy scipy\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Load the previously trained model\n",
        "# This path might need to be adjusted based on your environment\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp3/weights/best.pt')\n",
        "\n",
        "# Define the path to your video\n",
        "# You can use one of the traffic videos from your dataset\n",
        "video_path = '/kaggle/input/videotraffic/Traffic.mp4'\n",
        "\n",
        "# Check if the video exists\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"Video not found at {video_path}. Please update the path.\")\n",
        "    # Try to find a traffic video in the current environment\n",
        "    alternative_paths = [\n",
        "        '/content/Traffic.mp4',\n",
        "        '../input/videotraffic/Traffic.mp4',\n",
        "        './Traffic.mp4'\n",
        "    ]\n",
        "\n",
        "    for alt_path in alternative_paths:\n",
        "        if os.path.exists(alt_path):\n",
        "            video_path = alt_path\n",
        "            print(f\"Found alternative video at {video_path}\")\n",
        "            break\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(\"No suitable video found. Please download a traffic video or adjust the path.\")\n",
        "\n",
        "# Test regular tracking (from Milestone 3)\n",
        "print(\"Running standard tracking (Milestone 3)...\")\n",
        "# This assumes you've imported the track_and_predict_behavior function from Milestone 3\n",
        "standard_results = track_and_predict_behavior(\n",
        "    video_path,\n",
        "    model,\n",
        "    max_frames=20\n",
        ")\n",
        "\n",
        "# Test occlusion handling (from Milestone 4)\n",
        "print(\"\\nRunning occlusion-optimized tracking (Milestone 4)...\")\n",
        "occlusion_results = track_with_occlusion_handling(\n",
        "    video_path,\n",
        "    model,\n",
        "    max_frames=20\n",
        ")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nComparison of tracking results:\")\n",
        "print(f\"Standard tracking: {len(standard_results['trajectories'])} unique vehicles\")\n",
        "print(f\"Occlusion-optimized: {len(occlusion_results['trajectories'])} unique vehicles\")\n",
        "\n",
        "# Calculate average track length\n",
        "if standard_results['trajectories']:\n",
        "    std_avg_len = sum(len(traj) for traj in standard_results['trajectories'].values()) / len(standard_results['trajectories'])\n",
        "    print(f\"Standard tracking average track length: {std_avg_len:.2f} frames\")\n",
        "\n",
        "if occlusion_results['trajectories']:\n",
        "    occ_avg_len = sum(len(traj) for traj in occlusion_results['trajectories'].values()) / len(occlusion_results['trajectories'])\n",
        "    print(f\"Occlusion-optimized average track length: {occ_avg_len:.2f} frames\")\n",
        "\n",
        "# Create a synthetic occlusion test\n",
        "print(\"\\nCreating synthetic occlusion test...\")\n",
        "synthetic_test_path = \"synthetic_occlusion_test.mp4\"\n",
        "\n",
        "# Get video dimensions\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap.release()\n",
        "\n",
        "# Create the synthetic test\n",
        "create_synthetic_occlusion_test(\n",
        "    video_path,\n",
        "    synthetic_test_path,\n",
        "    occlusion_frames=[30, 60, 90],\n",
        "    occlusion_size=0.3\n",
        ")\n",
        "\n",
        "# Run tracking on the synthetic test\n",
        "print(\"\\nRunning tracking on synthetic occlusion test...\")\n",
        "synthetic_results = track_with_occlusion_handling(\n",
        "    synthetic_test_path,\n",
        "    model,\n",
        "    max_frames=120\n",
        ")\n",
        "\n",
        "# Visualize the occlusion handling results\n",
        "print(\"\\nVisualizing occlusion handling results...\")\n",
        "visualize_occlusion_results(synthetic_results, width, height)\n",
        "\n",
        "print(\"\\nTesting complete!\")"
      ],
      "metadata": {
        "id": "KlkUKDh2pMpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Milestone 5"
      ],
      "metadata": {
        "id": "8c3LRIQTt6YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "import queue\n",
        "import multiprocessing\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from scipy.signal import savgol_filter\n",
        "import cProfile\n",
        "import pstats\n",
        "import io\n",
        "\n",
        "# Step 11: Implement code profiling\n",
        "class PerformanceProfiler:\n",
        "    \"\"\"Class for profiling code performance\"\"\"\n",
        "\n",
        "    def __init__(self, enabled=True):\n",
        "        self.enabled = enabled\n",
        "        self.profiler = None\n",
        "        self.stats = None\n",
        "        self.function_timings = defaultdict(float)\n",
        "        self.section_timings = {}\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start profiling\"\"\"\n",
        "        if self.enabled:\n",
        "            self.profiler = cProfile.Profile()\n",
        "            self.profiler.enable()\n",
        "            return self\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop profiling and collect stats\"\"\"\n",
        "        if self.enabled and self.profiler:\n",
        "            self.profiler.disable()\n",
        "            s = io.StringIO()\n",
        "            ps = pstats.Stats(self.profiler, stream=s).sort_stats('cumulative')\n",
        "            ps.print_stats(30)  # Print top 30 functions by cumulative time\n",
        "            self.stats = s.getvalue()\n",
        "            return self.stats\n",
        "\n",
        "    def start_timer(self, section_name):\n",
        "        \"\"\"Start timing a specific section of code\"\"\"\n",
        "        if self.enabled:\n",
        "            self.section_timings[section_name] = time.time()\n",
        "\n",
        "    def stop_timer(self, section_name):\n",
        "        \"\"\"Stop timing a section and record the elapsed time\"\"\"\n",
        "        if self.enabled and section_name in self.section_timings:\n",
        "            elapsed = time.time() - self.section_timings[section_name]\n",
        "            self.function_timings[section_name] += elapsed\n",
        "            return elapsed\n",
        "        return 0\n",
        "\n",
        "    def get_function_timings(self):\n",
        "        \"\"\"Get the recorded timings for all functions\"\"\"\n",
        "        return dict(self.function_timings)\n",
        "\n",
        "    def reset_timings(self):\n",
        "        \"\"\"Reset all timing information\"\"\"\n",
        "        self.function_timings = defaultdict(float)\n",
        "        self.section_timings = {}\n",
        "\n",
        "    def print_timings(self):\n",
        "        \"\"\"Print the timing information in a formatted way\"\"\"\n",
        "        if not self.enabled:\n",
        "            return\n",
        "\n",
        "        print(\"\\n--- Function Timing Results ---\")\n",
        "        total_time = sum(self.function_timings.values())\n",
        "\n",
        "        # Sort by time (descending)\n",
        "        sorted_times = sorted(self.function_timings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for func_name, elapsed in sorted_times:\n",
        "            percentage = (elapsed / total_time * 100) if total_time > 0 else 0\n",
        "            print(f\"{func_name}: {elapsed:.4f}s ({percentage:.1f}%)\")\n",
        "\n",
        "        print(f\"Total measured time: {total_time:.4f}s\")\n",
        "\n",
        "    def visualize_timings(self):\n",
        "        \"\"\"Create a bar chart of function timings\"\"\"\n",
        "        if not self.enabled or not self.function_timings:\n",
        "            return\n",
        "\n",
        "        # Sort by time (descending)\n",
        "        sorted_times = sorted(self.function_timings.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_funcs = sorted_times[:10]  # Show top 10 functions\n",
        "\n",
        "        func_names = [name for name, _ in top_funcs]\n",
        "        times = [time_val for _, time_val in top_funcs]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bars = plt.bar(func_names, times, color='skyblue')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('Top 10 Functions by Execution Time')\n",
        "        plt.xlabel('Function')\n",
        "        plt.ylabel('Time (seconds)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Add time labels on top of bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{height:.3f}s', ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Step 12: Model Optimization Class\n",
        "class ModelOptimizer:\n",
        "    \"\"\"Class for optimizing YOLOv5 models for inference speed\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, device=None):\n",
        "        self.model_path = model_path\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.original_model = None\n",
        "\n",
        "    def load_model(self, quantized=False):\n",
        "        \"\"\"Load the YOLOv5 model, with optional quantization\"\"\"\n",
        "        print(f\"Loading model from {self.model_path} on {self.device}\")\n",
        "\n",
        "        # Load the original model first\n",
        "        self.original_model = torch.hub.load('ultralytics/yolov5', 'custom', path=self.model_path)\n",
        "\n",
        "        # If quantization is requested, apply it\n",
        "        if quantized and self.device == torch.device(\"cpu\"):\n",
        "            print(\"Applying quantization for CPU inference...\")\n",
        "            self.model = self._quantize_model(self.original_model)\n",
        "        else:\n",
        "            self.model = self.original_model\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def _quantize_model(self, model):\n",
        "        \"\"\"Apply quantization to the model for faster CPU inference\"\"\"\n",
        "        try:\n",
        "            # Set model to eval mode\n",
        "            model.eval()\n",
        "\n",
        "            # Apply static quantization\n",
        "            quantized_model = torch.quantization.quantize_dynamic(\n",
        "                model,\n",
        "                {torch.nn.Linear, torch.nn.Conv2d},\n",
        "                dtype=torch.qint8\n",
        "            )\n",
        "\n",
        "            print(\"Model quantized successfully\")\n",
        "            return quantized_model\n",
        "        except Exception as e:\n",
        "            print(f\"Quantization failed: {e}\")\n",
        "            # Return original model if quantization fails\n",
        "            return model\n",
        "\n",
        "    def switch_to_smaller_model(self, model_size='yolov5s'):\n",
        "        \"\"\"Switch to a smaller YOLOv5 model variant for speed\"\"\"\n",
        "        print(f\"Switching to smaller model: {model_size}\")\n",
        "        try:\n",
        "            # Load a smaller pre-trained model\n",
        "            smaller_model = torch.hub.load('ultralytics/yolov5', model_size)\n",
        "\n",
        "            # Transfer the class names from the original model\n",
        "            if hasattr(self.original_model, 'names'):\n",
        "                smaller_model.names = self.original_model.names\n",
        "\n",
        "            self.model = smaller_model\n",
        "            return self.model\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to switch to smaller model: {e}\")\n",
        "            return self.model\n",
        "\n",
        "    def optimize_inference_settings(self, model=None):\n",
        "        \"\"\"Optimize model inference settings\"\"\"\n",
        "        if model is None:\n",
        "            model = self.model\n",
        "\n",
        "        # Set inference settings\n",
        "        if model is not None:\n",
        "            model.conf = 0.25  # Confidence threshold\n",
        "            model.iou = 0.45   # IoU threshold\n",
        "            model.classes = None  # Filter by class, None = all classes\n",
        "            model.agnostic = False  # NMS agnostic (class-independent)\n",
        "            model.multi_label = False  # Multiple labels per box\n",
        "            model.max_det = 100  # Maximum detections\n",
        "\n",
        "        return model\n",
        "\n",
        "    def benchmark_inference(self, sample_image, iterations=20, warmup=5):\n",
        "        \"\"\"Benchmark model inference speed on a sample image\"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"No model loaded. Call load_model() first.\")\n",
        "            return\n",
        "\n",
        "        # Ensure the model is on the right device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Warmup runs\n",
        "        for _ in range(warmup):\n",
        "            _ = self.model(sample_image)\n",
        "\n",
        "        # Timed runs\n",
        "        start_time = time.time()\n",
        "        for _ in range(iterations):\n",
        "            _ = self.model(sample_image)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        avg_time = total_time / iterations\n",
        "        fps = iterations / total_time\n",
        "\n",
        "        print(f\"Model inference benchmark:\")\n",
        "        print(f\"  Average inference time: {avg_time*1000:.2f} ms\")\n",
        "        print(f\"  FPS: {fps:.2f}\")\n",
        "\n",
        "        return {\"avg_time\": avg_time, \"fps\": fps}\n",
        "\n",
        "# Step 13: Multi-threaded Video Processing\n",
        "class ThreadedVideoProcessor:\n",
        "    \"\"\"Class for multi-threaded video processing\"\"\"\n",
        "\n",
        "    def __init__(self, video_path, model, buffer_size=10):\n",
        "        self.video_path = video_path\n",
        "        self.model = model\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        # Create queues for frame passing between threads\n",
        "        self.frame_queue = queue.Queue(maxsize=buffer_size)\n",
        "        self.result_queue = queue.Queue(maxsize=buffer_size)\n",
        "\n",
        "        # Threading control\n",
        "        self.running = False\n",
        "        self.capture_thread = None\n",
        "        self.detection_thread = None\n",
        "\n",
        "        # Performance tracking\n",
        "        self.profiler = PerformanceProfiler()\n",
        "        self.fps_stats = {\"capture\": [], \"detection\": [], \"overall\": []}\n",
        "        self.last_frame_time = 0\n",
        "        self.frame_count = 0\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the threaded processing\"\"\"\n",
        "        if self.running:\n",
        "            print(\"Already running!\")\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "\n",
        "        # Start the threads\n",
        "        self.capture_thread = threading.Thread(target=self._capture_worker)\n",
        "        self.detection_thread = threading.Thread(target=self._detection_worker)\n",
        "\n",
        "        self.capture_thread.daemon = True\n",
        "        self.detection_thread.daemon = True\n",
        "\n",
        "        self.capture_thread.start()\n",
        "        self.detection_thread.start()\n",
        "\n",
        "        print(\"Threaded video processing started\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the threaded processing\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "        # Wait for threads to finish\n",
        "        if self.capture_thread and self.capture_thread.is_alive():\n",
        "            self.capture_thread.join(timeout=3.0)\n",
        "\n",
        "        if self.detection_thread and self.detection_thread.is_alive():\n",
        "            self.detection_thread.join(timeout=3.0)\n",
        "\n",
        "        # Clear queues\n",
        "        while not self.frame_queue.empty():\n",
        "            try:\n",
        "                self.frame_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "        while not self.result_queue.empty():\n",
        "            try:\n",
        "                self.result_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "        print(\"Threaded video processing stopped\")\n",
        "\n",
        "    def _capture_worker(self):\n",
        "        \"\"\"Worker thread for capturing frames from video\"\"\"\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video {self.video_path}\")\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        frame_count = 0\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        while self.running:\n",
        "            self.profiler.start_timer(\"capture\")\n",
        "\n",
        "            # Capture frame-by-frame\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"End of video stream\")\n",
        "                self.running = False\n",
        "                break\n",
        "\n",
        "            # Calculate capture FPS\n",
        "            current_time = time.time()\n",
        "            if self.last_frame_time > 0:\n",
        "                capture_fps = 1.0 / (current_time - self.last_frame_time)\n",
        "                self.fps_stats[\"capture\"].append(capture_fps)\n",
        "            self.last_frame_time = current_time\n",
        "\n",
        "            # Put frame in queue, with a timeout to prevent blocking forever\n",
        "            try:\n",
        "                self.frame_queue.put((frame_count, frame), timeout=1.0)\n",
        "                frame_count += 1\n",
        "            except queue.Full:\n",
        "                # Skip this frame if queue is full\n",
        "                pass\n",
        "\n",
        "            self.profiler.stop_timer(\"capture\")\n",
        "\n",
        "            # Add a small sleep to prevent CPU overuse\n",
        "            time.sleep(0.001)\n",
        "\n",
        "        # Release the video capture object\n",
        "        cap.release()\n",
        "\n",
        "    def _detection_worker(self):\n",
        "        \"\"\"Worker thread for running object detection\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Get a frame from the queue with timeout\n",
        "                frame_num, frame = self.frame_queue.get(timeout=1.0)\n",
        "\n",
        "                self.profiler.start_timer(\"detection\")\n",
        "\n",
        "                # Run detection\n",
        "                results = self.model(frame)\n",
        "\n",
        "                # Put the results in the result queue\n",
        "                detection_time = self.profiler.stop_timer(\"detection\")\n",
        "                detection_fps = 1.0 / detection_time if detection_time > 0 else 0\n",
        "                self.fps_stats[\"detection\"].append(detection_fps)\n",
        "\n",
        "                # Package results and metadata\n",
        "                result_package = {\n",
        "                    \"frame_num\": frame_num,\n",
        "                    \"frame\": frame,\n",
        "                    \"results\": results,\n",
        "                    \"detection_time\": detection_time,\n",
        "                    \"detection_fps\": detection_fps\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    self.result_queue.put(result_package, timeout=1.0)\n",
        "                except queue.Full:\n",
        "                    # If the result queue is full, drop the oldest result\n",
        "                    try:\n",
        "                        self.result_queue.get_nowait()\n",
        "                        self.result_queue.put(result_package, timeout=1.0)\n",
        "                    except (queue.Empty, queue.Full):\n",
        "                        pass\n",
        "\n",
        "                # Mark queue task as done\n",
        "                self.frame_queue.task_done()\n",
        "\n",
        "            except queue.Empty:\n",
        "                # No frames available, wait a bit\n",
        "                time.sleep(0.01)\n",
        "            except Exception as e:\n",
        "                print(f\"Error in detection worker: {e}\")\n",
        "                continue\n",
        "\n",
        "    def get_next_result(self, timeout=1.0):\n",
        "        \"\"\"Get the next processed frame and detection results\"\"\"\n",
        "        try:\n",
        "            return self.result_queue.get(timeout=timeout)\n",
        "        except queue.Empty:\n",
        "        # Function to run the traffic monitoring system with the optimal configuration\n",
        "def run_optimized_traffic_monitoring(video_path, model_path, output_path=None, max_frames=None):\n",
        "    \"\"\"\n",
        "    Run the traffic monitoring system with optimized settings\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the input video\n",
        "        model_path: Path to the YOLOv5 model weights\n",
        "        output_path: Path to save the output video (optional)\n",
        "        max_frames: Maximum number of frames to process (optional)\n",
        "\n",
        "    Returns:\n",
        "        Performance statistics dictionary\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Running Optimized Traffic Monitoring System ===\")\n",
        "\n",
        "    # First run a quick benchmark to find optimal settings\n",
        "    print(\"Running quick benchmark to find optimal configuration...\")\n",
        "    benchmark_result = benchmark_optimized_system(\n",
        "        video_path=video_path,\n",
        "        model_path=model_path,\n",
        "        max_frames=50  # Use a small number of frames for quick benchmarking\n",
        "    )\n",
        "\n",
        "    # Extract the best configuration\n",
        "    use_threading = benchmark_result['threading'] if benchmark_result else True\n",
        "    use_quantization = benchmark_result['quantization'] if benchmark_result else True\n",
        "\n",
        "    print(f\"\\nUsing optimal configuration: Threading={use_threading}, Quantization={use_quantization}\")\n",
        "\n",
        "    # Initialize with the optimal configuration\n",
        "    processor = OptimizedTrafficProcessor(\n",
        "        video_path=video_path,\n",
        "        model_path=model_path,\n",
        "        output_path=output_path,\n",
        "        use_threading=use_threading,\n",
        "        use_quantization=use_quantization\n",
        "    )\n",
        "\n",
        "    # Process the video\n",
        "    print(f\"\\nProcessing video: {video_path}\")\n",
        "    print(f\"Output path: {output_path if output_path else 'None'}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    processor.initialize()\n",
        "    processor.process_video(max_frames=max_frames)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    if processor.processing_times:\n",
        "        avg_processing_time = np.mean(processor.processing_times)\n",
        "        avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "    else:\n",
        "        avg_processing_time = 0\n",
        "        avg_fps = 0\n",
        "\n",
        "    # Return performance statistics\n",
        "    stats = {\n",
        "        \"total_time\": total_time,\n",
        "        \"frames_processed\": processor.frame_count,\n",
        "        \"avg_processing_time\": avg_processing_time,\n",
        "        \"avg_fps\": avg_fps,\n",
        "        \"total_unique_vehicles\": processor.total_unique_vehicles,\n",
        "        \"configuration\": {\n",
        "            \"threading\": use_threading,\n",
        "            \"quantization\": use_quantization\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Processing Complete ===\")\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    print(f\"Frames processed: {processor.frame_count}\")\n",
        "    print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "    print(f\"Unique vehicles detected: {processor.total_unique_vehicles}\")\n",
        "\n",
        "    if output_path:\n",
        "        print(f\"Output video saved to: {output_path}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Main entry point for demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description=\"Optimized Traffic Monitoring System\")\n",
        "    parser.add_argument(\"--video\", type=str, required=True, help=\"Path to input video\")\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to YOLOv5 model weights\")\n",
        "    parser.add_argument(\"--output\", type=str, default=None, help=\"Path to save output video\")\n",
        "    parser.add_argument(\"--max-frames\", type=int, default=None, help=\"Maximum frames to process\")\n",
        "    parser.add_argument(\"--benchmark-only\", action=\"store_true\", help=\"Run only the benchmark\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.benchmark_only:\n",
        "        # Run benchmark only\n",
        "        benchmark_optimized_system(\n",
        "            video_path=args.video,\n",
        "            model_path=args.model,\n",
        "            max_frames=args.max_frames if args.max_frames else 100\n",
        "        )\n",
        "    else:\n",
        "        # Run the full system\n",
        "        run_optimized_traffic_monitoring(\n",
        "            video_path=args.video,\n",
        "            model_path=args.model,\n",
        "            output_path=args.output,\n",
        "            max_frames=args.max_frames\n",
        "        )\n",
        "\n",
        "    def get_fps_stats(self):\n",
        "        \"\"\"Get the FPS statistics\"\"\"\n",
        "        capture_fps = np.mean(self.fps_stats[\"capture\"]) if self.fps_stats[\"capture\"] else 0\n",
        "        detection_fps = np.mean(self.fps_stats[\"detection\"]) if self.fps_stats[\"detection\"] else 0\n",
        "        overall_fps = min(capture_fps, detection_fps) if capture_fps > 0 and detection_fps > 0 else 0\n",
        "\n",
        "        return {\n",
        "            \"capture_fps\": capture_fps,\n",
        "            \"detection_fps\": detection_fps,\n",
        "            \"overall_fps\": overall_fps\n",
        "        }\n",
        "\n",
        "# Step 14: Optimized Processing Pipeline\n",
        "class OptimizedTrafficProcessor:\n",
        "    \"\"\"Main class for optimized traffic video processing\"\"\"\n",
        "\n",
        "    def __init__(self, video_path, model_path, output_path=None, use_threading=True, use_quantization=True):\n",
        "        self.video_path = video_path\n",
        "        self.model_path = model_path\n",
        "        self.output_path = output_path\n",
        "        self.use_threading = use_threading\n",
        "        self.use_quantization = use_quantization\n",
        "\n",
        "        # Components initialization\n",
        "        self.profiler = PerformanceProfiler()\n",
        "        self.model_optimizer = None\n",
        "        self.threaded_processor = None\n",
        "        self.tracker = None\n",
        "        self.video_writer = None\n",
        "\n",
        "        # Tracking data\n",
        "        self.trajectories = defaultdict(list)\n",
        "        self.speeds = defaultdict(float)\n",
        "        self.behaviors = defaultdict(str)\n",
        "        self.violations = defaultdict(bool)\n",
        "\n",
        "        # Performance metrics\n",
        "        self.processing_times = []\n",
        "        self.frame_count = 0\n",
        "        self.total_vehicles = 0\n",
        "        self.total_unique_vehicles = 0\n",
        "\n",
        "        # Visualization settings\n",
        "        self.colors = np.random.randint(0, 255, size=(100, 3), dtype=np.uint8)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize components\"\"\"\n",
        "        # Start profiling\n",
        "        self.profiler.start()\n",
        "\n",
        "        # Initialize model optimizer\n",
        "        self.profiler.start_timer(\"model_initialization\")\n",
        "        self.model_optimizer = ModelOptimizer(self.model_path)\n",
        "        model = self.model_optimizer.load_model(quantized=self.use_quantization)\n",
        "        model = self.model_optimizer.optimize_inference_settings(model)\n",
        "        self.profiler.stop_timer(\"model_initialization\")\n",
        "\n",
        "        # Initialize DeepSORT tracker with optimized settings\n",
        "        self.profiler.start_timer(\"tracker_initialization\")\n",
        "        self.tracker = DeepSort(\n",
        "            max_age=30,\n",
        "            n_init=3,\n",
        "            nn_budget=100,\n",
        "            embedder=\"mobilenet\",\n",
        "            nms_max_overlap=1.0,\n",
        "            max_cosine_distance=0.2\n",
        "        )\n",
        "        self.profiler.stop_timer(\"tracker_initialization\")\n",
        "\n",
        "        # Initialize threaded processor if enabled\n",
        "        if self.use_threading:\n",
        "            self.threaded_processor = ThreadedVideoProcessor(\n",
        "                self.video_path, model, buffer_size=30\n",
        "            )\n",
        "\n",
        "        # Get video properties\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "        self.fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        self.frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        self.frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.release()\n",
        "\n",
        "        # Initialize video writer if output path is provided\n",
        "        if self.output_path:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            self.video_writer = cv2.VideoWriter(\n",
        "                self.output_path, fourcc, self.fps,\n",
        "                (self.frame_width, self.frame_height)\n",
        "            )\n",
        "\n",
        "        print(f\"Initialized with threading={self.use_threading}, quantization={self.use_quantization}\")\n",
        "        print(f\"Video properties: {self.frame_width}x{self.frame_height} @ {self.fps} FPS\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def process_video(self, max_frames=None):\n",
        "        \"\"\"Process the video with the optimized pipeline\"\"\"\n",
        "        if not self.model_optimizer or not self.model_optimizer.model:\n",
        "            print(\"Model not initialized. Call initialize() first.\")\n",
        "            return\n",
        "\n",
        "        model = self.model_optimizer.model\n",
        "\n",
        "        # Start the threaded processor if enabled\n",
        "        if self.use_threading and self.threaded_processor:\n",
        "            self.threaded_processor.start()\n",
        "            processing_method = self._process_threaded\n",
        "        else:\n",
        "            # Open the video file for non-threaded processing\n",
        "            cap = cv2.VideoCapture(self.video_path)\n",
        "            if not cap.isOpened():\n",
        "                print(f\"Error: Could not open video {self.video_path}\")\n",
        "                return\n",
        "            processing_method = lambda: self._process_synchronous(cap, model)\n",
        "\n",
        "        # Process frames\n",
        "        try:\n",
        "            processing_method()\n",
        "\n",
        "            # Apply max_frames limit if specified\n",
        "            if max_frames and self.frame_count >= max_frames:\n",
        "                print(f\"Reached maximum frame limit ({max_frames})\")\n",
        "\n",
        "        finally:\n",
        "            # Clean up resources\n",
        "            if self.use_threading and self.threaded_processor:\n",
        "                self.threaded_processor.stop()\n",
        "            else:\n",
        "                cap.release()\n",
        "\n",
        "            if self.video_writer:\n",
        "                self.video_writer.release()\n",
        "\n",
        "            # Stop profiling and print results\n",
        "            self.profiler.stop()\n",
        "            self.profiler.print_timings()\n",
        "            self.profiler.visualize_timings()\n",
        "\n",
        "            # Print summary statistics\n",
        "            self._print_summary()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _process_threaded(self):\n",
        "        \"\"\"Process video using the threaded processor\"\"\"\n",
        "        while True:\n",
        "            # Get the next processed frame and detection results\n",
        "            result_package = self.threaded_processor.get_next_result()\n",
        "\n",
        "            if result_package is None:\n",
        "                # No more frames or timeout\n",
        "                if not self.threaded_processor.running:\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            frame_num = result_package[\"frame_num\"]\n",
        "            frame = result_package[\"frame\"]\n",
        "            results = result_package[\"results\"]\n",
        "\n",
        "            # Process the detection results\n",
        "            self._process_frame(frame, results)\n",
        "\n",
        "            # Update frame count\n",
        "            self.frame_count += 1\n",
        "\n",
        "    def _process_synchronous(self, cap, model):\n",
        "        \"\"\"Process video synchronously (without threading)\"\"\"\n",
        "        while True:\n",
        "            self.profiler.start_timer(\"frame_read\")\n",
        "            ret, frame = cap.read()\n",
        "            frame_read_time = self.profiler.stop_timer(\"frame_read\")\n",
        "\n",
        "            if not ret:\n",
        "                print(\"End of video\")\n",
        "                break\n",
        "\n",
        "            # Process frame\n",
        "            self.profiler.start_timer(\"inference\")\n",
        "            results = model(frame)\n",
        "            inference_time = self.profiler.stop_timer(\"inference\")\n",
        "\n",
        "            # Process the detection results\n",
        "            self._process_frame(frame, results)\n",
        "\n",
        "            # Update frame count\n",
        "            self.frame_count += 1\n",
        "\n",
        "    def _process_frame(self, frame, results):\n",
        "        \"\"\"Process a frame and its detection results\"\"\"\n",
        "        # Start timing frame processing\n",
        "        self.profiler.start_timer(\"frame_processing\")\n",
        "\n",
        "        # 1. Get detections from results\n",
        "        self.profiler.start_timer(\"detections_extraction\")\n",
        "        detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "        # Format detections for DeepSORT\n",
        "        detection_list = []\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            detection_list.append(\n",
        "                ([x1, y1, x2 - x1, y2 - y1], conf, int(cls))\n",
        "            )\n",
        "        self.profiler.stop_timer(\"detections_extraction\")\n",
        "\n",
        "        # 2. Update tracker\n",
        "        self.profiler.start_timer(\"tracking\")\n",
        "        tracks = self.tracker.update_tracks(detection_list, frame=frame)\n",
        "        active_tracks = [track for track in tracks if track.is_confirmed()]\n",
        "        self.profiler.stop_timer(\"tracking\")\n",
        "\n",
        "        # 3. Update tracking statistics\n",
        "        frame_vehicle_count = len(active_tracks)\n",
        "        self.total_vehicles += frame_vehicle_count\n",
        "\n",
        "        # 4. Process each active track\n",
        "        self.profiler.start_timer(\"track_processing\")\n",
        "        processed_frame = self._process_tracks(frame, active_tracks)\n",
        "        self.profiler.stop_timer(\"track_processing\")\n",
        "\n",
        "        # 5. Write frame if output is enabled\n",
        "        if self.video_writer:\n",
        "            self.profiler.start_timer(\"frame_write\")\n",
        "            self.video_writer.write(processed_frame)\n",
        "            self.profiler.stop_timer(\"frame_write\")\n",
        "\n",
        "        # Calculate processing time for this frame\n",
        "        frame_processing_time = self.profiler.stop_timer(\"frame_processing\")\n",
        "        self.processing_times.append(frame_processing_time)\n",
        "\n",
        "        # Display frame or save for visualization\n",
        "        self._visualize_frame(processed_frame, frame_processing_time)\n",
        "\n",
        "        return processed_frame\n",
        "\n",
        "    def _process_tracks(self, frame, active_tracks):\n",
        "        \"\"\"Process all active tracks and draw visualization\"\"\"\n",
        "        # Create a copy of the frame for drawing\n",
        "        processed_frame = frame.copy()\n",
        "\n",
        "        # Draw tracking results on the frame\n",
        "        for track in active_tracks:\n",
        "            track_id = track.track_id\n",
        "            bbox = track.to_ltrb()\n",
        "            x1, y1, x2, y2 = bbox\n",
        "\n",
        "            # Get track position (center of bounding box)\n",
        "            center_x = int((x1 + x2) / 2)\n",
        "            center_y = int((y1 + y2) / 2)\n",
        "            current_pos = (center_x, center_y)\n",
        "\n",
        "            # Add point to trajectory\n",
        "            self.trajectories[track_id].append(current_pos)\n",
        "\n",
        "            # Update speed estimate\n",
        "            self.speeds[track_id] = self._estimate_speed(self.trajectories[track_id], self.fps)\n",
        "\n",
        "            # Count unique vehicles\n",
        "            if len(self.trajectories[track_id]) == 1:\n",
        "                self.total_unique_vehicles += 1\n",
        "\n",
        "            # Get color for this track ID\n",
        "            color = self.colors[track_id % len(self.colors)].tolist()\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(processed_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "\n",
        "            # Add ID and speed text\n",
        "            speed_text = f\"ID:{track_id} Speed:{self.speeds[track_id]:.1f}\"\n",
        "            cv2.putText(processed_frame, speed_text, (int(x1), int(y1)-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Draw trajectory\n",
        "            if len(self.trajectories[track_id]) > 1:\n",
        "                # Draw the trajectory line (last 20 points max for efficiency)\n",
        "                traj_points = self.trajectories[track_id][-20:]\n",
        "                for i in range(1, len(traj_points)):\n",
        "                    pt1 = traj_points[i-1]\n",
        "                    pt2 = traj_points[i]\n",
        "                    cv2.line(processed_frame, pt1, pt2, color, 2)\n",
        "\n",
        "        return processed_frame\n",
        "\n",
        "    def _estimate_speed(self, traj_points, fps):\n",
        "        \"\"\"Estimate vehicle speed based on trajectory points\"\"\"\n",
        "        if len(traj_points) < 5:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate displacement over last 5 points\n",
        "        distances = []\n",
        "        for i in range(1, min(6, len(traj_points))):\n",
        "            distances.append(self._calculate_distance(traj_points[-i], traj_points[-i-1]))\n",
        "\n",
        "        # Average displacement per frame\n",
        "        avg_displacement = sum(distances) / len(distances)\n",
        "\n",
        "        # Convert to speed (pixels per second)\n",
        "        speed = avg_displacement * fps\n",
        "\n",
        "        return speed\n",
        "\n",
        "    def _calculate_distance(self, point1, point2):\n",
        "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
        "        return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
        "\n",
        "    def _visualize_frame(self, frame, processing_time):\n",
        "        \"\"\"Visualize a processed frame\"\"\"\n",
        "        # Calculate FPS\n",
        "        fps_current = 1.0 / processing_time if processing_time > 0 else 0\n",
        "\n",
        "        # Add FPS text to the frame\n",
        "        cv2.putText(frame, f\"FPS: {fps_current:.2f}\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add tracking info\n",
        "        cv2.putText(frame, f\"Vehicles: {len(self.trajectories)}\", (10, 60),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Add optimization info\n",
        "        threading_text = \"Threading: ON\" if self.use_threading else \"Threading: OFF\"\n",
        "        cv2.putText(frame, threading_text, (10, 90),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "        # Show frame using matplotlib or OpenCV depending on environment\n",
        "        # This can be customized based on your specific display needs\n",
        "        if self.frame_count % 10 == 0:  # Show every 10th frame for efficiency\n",
        "            # Convert BGR to RGB for matplotlib\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.imshow(rgb_frame)\n",
        "            plt.title(f\"Frame {self.frame_count} | FPS: {fps_current:.2f}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print summary statistics after processing\"\"\"\n",
        "        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0\n",
        "        avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "        print(\"\\n--- Processing Summary ---\")\n",
        "        print(f\"Total frames processed: {self.frame_count}\")\n",
        "        print(f\"Average processing time: {avg_processing_time*1000:.2f} ms per frame\")\n",
        "        print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "        print(f\"Total unique vehicles: {self.total_unique_vehicles}\")\n",
        "        print(f\"Average vehicles per frame: {self.total_vehicles / self.frame_count:.2f}\" if self.frame_count else \"No frames processed\")\n",
        "\n",
        "        if self.use_threading and self.threaded_processor:\n",
        "            fps_stats = self.threaded_processor.get_fps_stats()\n",
        "            print(\"\\n--- Threading Performance ---\")\n",
        "            print(f\"Capture FPS: {fps_stats['capture_fps']:.2f}\")\n",
        "            print(f\"Detection FPS: {fps_stats['detection_fps']:.2f}\")\n",
        "            print(f\"Overall FPS: {fps_stats['overall_fps']:.2f}\")\n",
        "\n",
        "        # Plot FPS over time\n",
        "        if self.processing_times:\n",
        "            fps_values = [1.0/t if t > 0 else 0 for t in self.processing_times]\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(fps_values, 'b-')\n",
        "            plt.axhline(y=avg_fps, color='r', linestyle='--', label=f'Average: {avg_fps:.2f} FPS')\n",
        "            plt.title('Processing FPS over Time')\n",
        "            plt.xlabel('Frame')\n",
        "            plt.ylabel('FPS')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Step 15: Benchmarking Function\n",
        "def benchmark_optimized_system(video_path, model_path, max_frames=100):\n",
        "    \"\"\"Benchmark the optimized system with different configurations\"\"\"\n",
        "    print(\"\\n--- Benchmarking Optimized Traffic Monitoring System ---\\n\")\n",
        "\n",
        "    configs = [\n",
        "        {\"name\": \"Baseline (no optimizations)\", \"threading\": False, \"quantization\": False},\n",
        "        {\"name\": \"Threading only\", \"threading\": True, \"quantization\": False},\n",
        "        {\"name\": \"Quantization only\", \"threading\": False, \"quantization\": True},\n",
        "        {\"name\": \"Full optimization\", \"threading\": True, \"quantization\": True}\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"\\nTesting configuration: {config['name']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        processor = OptimizedTrafficProcessor(\n",
        "            video_path=video_path,\n",
        "            model_path=model_path,\n",
        "            use_threading=config['threading'],\n",
        "            use_quantization=config['quantization']\n",
        "        )\n",
        "\n",
        "        # Initialize and process\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            processor.initialize()\n",
        "            processor.process_video(max_frames=max_frames)\n",
        "\n",
        "            # Calculate metrics\n",
        "            processing_time = time.time() - start_time\n",
        "            avg_processing_time = np.mean(processor.processing_times) if processor.processing_times else 0\n",
        "            avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"config\": config['name'],\n",
        "                \"threading\": config['threading'],\n",
        "                \"quantization\": config['quantization'],\n",
        "                \"total_time\": processing_time,\n",
        "                \"avg_time_per_frame\": avg_processing_time,\n",
        "                \"avg_fps\": avg_fps,\n",
        "                \"vehicle_count\": processor.total_unique_vehicles\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during benchmark: {e}\")\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n--- Benchmark Results ---\")\n",
        "    print(\"Configuration | Threading | Quantization | Total Time (s) | Avg FPS\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"{result['config']:<20} | {str(result['threading']):<10} | \"\n",
        "              f\"{str(result['quantization']):<12} | {result['total_time']:<14.2f} | {result['avg_fps']:<7.2f}\")\n",
        "\n",
        "    # Plot comparison\n",
        "    if results:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # FPS comparison\n",
        "        plt.subplot(1, 2, 1)\n",
        "        names = [r['config'] for r in results]\n",
        "        fps_values = [r['avg_fps'] for r in results]\n",
        "\n",
        "        bars = plt.bar(names, fps_values, color='skyblue')\n",
        "        plt.title('Average FPS by Configuration')\n",
        "        plt.ylabel('Frames Per Second')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add FPS values on top of bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{height:.2f}', ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Total processing time comparison\n",
        "        plt.subplot(1, 2, 2)\n",
        "        times = [r['total_time'] for r in results]\n",
        "\n",
        "        bars = plt.bar(names, times, color='salmon')\n",
        "        plt.title('Total Processing Time by Configuration')\n",
        "        plt.ylabel('Time (seconds)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add time values on top of bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{height:.2f}s', ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Return the best configuration based on FPS\n",
        "    if results:\n",
        "        best_result = max(results, key=lambda x: x['avg_fps'])\n",
        "        print(f\"\\nBest configuration: {best_result['config']} with {best_result['avg_fps']:.2f} FPS\")\n",
        "        return best_result\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "qTuXxfjvt5yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import our optimized traffic processing system\n",
        "from milestone5_optimization import (\n",
        "    PerformanceProfiler,\n",
        "    ModelOptimizer,\n",
        "    ThreadedVideoProcessor,\n",
        "    OptimizedTrafficProcessor,\n",
        "    benchmark_optimized_system\n",
        ")\n",
        "\n",
        "# Define paths\n",
        "video_path = \"/kaggle/input/videotraffic/Traffic.mp4\"  # Update this to your video path\n",
        "model_path = \"runs/train/exp3/weights/best.pt\"         # Update this to your model path\n",
        "output_path = \"optimized_output.mp4\"                   # Output video path\n",
        "\n",
        "# Example 1: Simple usage with default settings\n",
        "def example_basic_usage():\n",
        "    print(\"Running basic usage example...\")\n",
        "\n",
        "    # Initialize the processor\n",
        "    processor = OptimizedTrafficProcessor(\n",
        "        video_path=video_path,\n",
        "        model_path=model_path,\n",
        "        output_path=output_path\n",
        "    )\n",
        "\n",
        "    # Initialize and process video\n",
        "    processor.initialize()\n",
        "    processor.process_video(max_frames=100)  # Process first 100 frames\n",
        "\n",
        "    print(\"Basic usage example completed.\")\n",
        "\n",
        "# Example 2: Benchmark different configurations\n",
        "def example_benchmark():\n",
        "    print(\"Running benchmark example...\")\n",
        "\n",
        "    # Run the benchmark with 100 frames\n",
        "    best_config = benchmark_optimized_system(\n",
        "        video_path=video_path,\n",
        "        model_path=model_path,\n",
        "        max_frames=100\n",
        "    )\n",
        "\n",
        "    print(f\"Benchmark completed. Best configuration: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "# Example 3: Model optimization only\n",
        "def example_model_optimization():\n",
        "    print(\"Running model optimization example...\")\n",
        "\n",
        "    # Initialize the model optimizer\n",
        "    model_optimizer = ModelOptimizer(model_path)\n",
        "\n",
        "    # Load the model with quantization\n",
        "    model = model_optimizer.load_model(quantized=True)\n",
        "\n",
        "    # Try the smaller model variant\n",
        "    smaller_model = model_optimizer.switch_to_smaller_model(model_size='yolov5s')\n",
        "\n",
        "    # Benchmark the models\n",
        "    print(\"\\nBenchmarking original model:\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, sample_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if ret:\n",
        "        model_optimizer.benchmark_inference(sample_frame)\n",
        "\n",
        "        print(\"\\nBenchmarking smaller model:\")\n",
        "        model_optimizer.model = smaller_model\n",
        "        model_optimizer.benchmark_inference(sample_frame)\n",
        "    else:\n",
        "        print(\"Failed to read a frame from the video for benchmarking.\")\n",
        "\n",
        "    print(\"Model optimization example completed.\")\n",
        "    return model_optimizer\n",
        "\n",
        "# Example 4: Profiling only\n",
        "def example_profiling():\n",
        "    print(\"Running profiling example...\")\n",
        "\n",
        "    # Initialize profiler\n",
        "    profiler = PerformanceProfiler()\n",
        "    profiler.start()\n",
        "\n",
        "    # Run some operations to profile\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
        "\n",
        "    # Time the model inference\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames_to_process = 20\n",
        "\n",
        "    for i in range(frames_to_process):\n",
        "        profiler.start_timer(f\"frame_{i}\")\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        profiler.start_timer(f\"inference_{i}\")\n",
        "        results = model(frame)\n",
        "        profiler.stop_timer(f\"inference_{i}\")\n",
        "\n",
        "        # Do some additional processing\n",
        "        profiler.start_timer(f\"post_processing_{i}\")\n",
        "        detections = results.xyxy[0].cpu().numpy()\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            # Draw bounding boxes (just for profiling purposes)\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        profiler.stop_timer(f\"post_processing_{i}\")\n",
        "\n",
        "        profiler.stop_timer(f\"frame_{i}\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Stop profiling and print results\n",
        "    stats = profiler.stop()\n",
        "    print(\"\\nProfiling results:\")\n",
        "    print(stats)\n",
        "\n",
        "    # Print and visualize function timings\n",
        "    profiler.print_timings()\n",
        "    profiler.visualize_timings()\n",
        "\n",
        "    print(\"Profiling example completed.\")\n",
        "    return profiler\n",
        "\n",
        "# Example 5: Run optimized system with best configuration\n",
        "def example_run_optimized(use_threading=True, use_quantization=True, max_frames=100):\n",
        "    print(f\"Running optimized system with threading={use_threading}, quantization={use_quantization}...\")\n",
        "\n",
        "    # Initialize the processor with best configuration\n",
        "    processor = OptimizedTrafficProcessor(\n",
        "        video_path=video_path,\n",
        "        model_path=model_path,\n",
        "        output_path=output_path,\n",
        "        use_threading=use_threading,\n",
        "        use_quantization=use_quantization\n",
        "    )\n",
        "\n",
        "    # Initialize and process video\n",
        "    processor.initialize()\n",
        "    processor.process_video(max_frames=max_frames)\n",
        "\n",
        "    print(\"Optimized system run completed.\")\n",
        "    return processor\n",
        "\n",
        "# Main function to run examples\n",
        "def main():\n",
        "    print(\"Milestone 5: Real-Time Optimization Examples\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Uncomment the examples you want to run\n",
        "\n",
        "    # Example 1: Basic usage\n",
        "    # example_basic_usage()\n",
        "\n",
        "    # Example 2: Benchmark different configurations\n",
        "    best_config = example_benchmark()\n",
        "\n",
        "    # Example 3: Model optimization only\n",
        "    # model_optimizer = example_model_optimization()\n",
        "\n",
        "    # Example 4: Profiling only\n",
        "    # profiler = example_profiling()\n",
        "\n",
        "    # Example 5: Run with best configuration from benchmark\n",
        "    if best_config:\n",
        "        example_run_optimized(\n",
        "            use_threading=best_config.get('threading', True),\n",
        "            use_quantization=best_config.get('quantization', True),\n",
        "            max_frames=200\n",
        "        )\n",
        "    else:\n",
        "        # Use default configuration if benchmark wasn't run\n",
        "        example_run_optimized()\n",
        "\n",
        "    print(\"\\nAll examples completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XdShcbStuEws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Version"
      ],
      "metadata": {
        "id": "U9BQuuPLwWuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import threading\n",
        "import queue\n",
        "import math\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from collections import defaultdict\n",
        "from scipy.signal import savgol_filter\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Global configuration\n",
        "CONFIG = {\n",
        "    \"use_threading\": True,           # Use multi-threading for improved performance\n",
        "    \"use_quantization\": True,        # Use model quantization for faster CPU inference\n",
        "    \"display_results\": True,         # Show visualizations during processing\n",
        "    \"save_output\": True,             # Save output video\n",
        "    \"show_every_nth_frame\": 10,      # Show visualization every Nth frame (higher = faster)\n",
        "    \"confidence_threshold\": 0.3,     # Detection confidence threshold\n",
        "    \"max_frames\": None,              # Maximum frames to process (None = all frames)\n",
        "    \"enable_behavior_prediction\": True,  # Enable behavior prediction (Milestone 3)\n",
        "    \"enable_occlusion_handling\": True,   # Enable occlusion handling (Milestone 4)\n",
        "    \"enable_profiling\": False,        # Enable detailed performance profiling\n",
        "}\n",
        "\n",
        "class PerformanceTracker:\n",
        "    \"\"\"Simple performance tracking for timing code sections\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.timings = defaultdict(list)\n",
        "        self.start_times = {}\n",
        "\n",
        "    def start(self, section_name):\n",
        "        \"\"\"Start timing a section\"\"\"\n",
        "        self.start_times[section_name] = time.time()\n",
        "\n",
        "    def stop(self, section_name):\n",
        "        \"\"\"Stop timing a section and record the elapsed time\"\"\"\n",
        "        if section_name in self.start_times:\n",
        "            elapsed = time.time() - self.start_times[section_name]\n",
        "            self.timings[section_name].append(elapsed)\n",
        "            del self.start_times[section_name]\n",
        "            return elapsed\n",
        "        return 0\n",
        "\n",
        "    def get_avg_timing(self, section_name):\n",
        "        \"\"\"Get average timing for a section\"\"\"\n",
        "        if section_name in self.timings and self.timings[section_name]:\n",
        "            return sum(self.timings[section_name]) / len(self.timings[section_name])\n",
        "        return 0\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print timing summary\"\"\"\n",
        "        print(\"\\n----- Performance Summary -----\")\n",
        "        for section, times in self.timings.items():\n",
        "            if times:\n",
        "                avg_time = sum(times) / len(times)\n",
        "                max_time = max(times)\n",
        "                min_time = min(times)\n",
        "                print(f\"{section}: avg={avg_time*1000:.2f}ms, min={min_time*1000:.2f}ms, max={max_time*1000:.2f}ms\")\n",
        "\n",
        "class ThreadedVideoProcessor:\n",
        "    \"\"\"Multi-threaded video processor for improved performance\"\"\"\n",
        "\n",
        "    def __init__(self, video_path, model, buffer_size=10):\n",
        "        self.video_path = video_path\n",
        "        self.model = model\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "        # Create queues for frame passing between threads\n",
        "        self.frame_queue = queue.Queue(maxsize=buffer_size)\n",
        "        self.result_queue = queue.Queue(maxsize=buffer_size)\n",
        "\n",
        "        # Threading control\n",
        "        self.running = False\n",
        "        self.capture_thread = None\n",
        "        self.detection_thread = None\n",
        "\n",
        "        # Video properties\n",
        "        self.cap = cv2.VideoCapture(video_path)\n",
        "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
        "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        self.cap.release()  # Release for now, will reopen in thread\n",
        "\n",
        "        # Performance tracking\n",
        "        self.performance = PerformanceTracker()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the threaded processing\"\"\"\n",
        "        if self.running:\n",
        "            print(\"Already running!\")\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "\n",
        "        # Start the threads\n",
        "        self.capture_thread = threading.Thread(target=self._capture_worker)\n",
        "        self.detection_thread = threading.Thread(target=self._detection_worker)\n",
        "\n",
        "        self.capture_thread.daemon = True\n",
        "        self.detection_thread.daemon = True\n",
        "\n",
        "        self.capture_thread.start()\n",
        "        self.detection_thread.start()\n",
        "\n",
        "        print(\"Threaded video processing started\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the threaded processing\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "        # Wait for threads to finish\n",
        "        if self.capture_thread and self.capture_thread.is_alive():\n",
        "            self.capture_thread.join(timeout=3.0)\n",
        "\n",
        "        if self.detection_thread and self.detection_thread.is_alive():\n",
        "            self.detection_thread.join(timeout=3.0)\n",
        "\n",
        "        # Clear queues\n",
        "        while not self.frame_queue.empty():\n",
        "            try:\n",
        "                self.frame_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "        while not self.result_queue.empty():\n",
        "            try:\n",
        "                self.result_queue.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "        print(\"Threaded video processing stopped\")\n",
        "\n",
        "    def _capture_worker(self):\n",
        "        \"\"\"Worker thread for capturing frames from video\"\"\"\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video {self.video_path}\")\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        frame_count = 0\n",
        "\n",
        "        while self.running:\n",
        "            self.performance.start(\"capture\")\n",
        "\n",
        "            # Capture frame-by-frame\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"End of video stream\")\n",
        "                self.running = False\n",
        "                break\n",
        "\n",
        "            # Put frame in queue, with a timeout to prevent blocking forever\n",
        "            try:\n",
        "                self.frame_queue.put((frame_count, frame), timeout=1.0)\n",
        "                frame_count += 1\n",
        "            except queue.Full:\n",
        "                # Skip this frame if queue is full\n",
        "                pass\n",
        "\n",
        "            self.performance.stop(\"capture\")\n",
        "\n",
        "            # Add a small sleep to prevent CPU overuse\n",
        "            time.sleep(0.001)\n",
        "\n",
        "        # Release the video capture object\n",
        "        cap.release()\n",
        "\n",
        "    def _detection_worker(self):\n",
        "        \"\"\"Worker thread for running object detection\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Get a frame from the queue with timeout\n",
        "                frame_num, frame = self.frame_queue.get(timeout=1.0)\n",
        "\n",
        "                self.performance.start(\"detection\")\n",
        "\n",
        "                # Run detection\n",
        "                results = self.model(frame)\n",
        "\n",
        "                # Calculate detection time and FPS\n",
        "                detection_time = self.performance.stop(\"detection\")\n",
        "                detection_fps = 1.0 / detection_time if detection_time > 0 else 0\n",
        "\n",
        "                # Package results and metadata\n",
        "                result_package = {\n",
        "                    \"frame_num\": frame_num,\n",
        "                    \"frame\": frame.copy(),  # Create a copy to avoid reference issues\n",
        "                    \"results\": results,\n",
        "                    \"detection_time\": detection_time,\n",
        "                    \"detection_fps\": detection_fps\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    self.result_queue.put(result_package, timeout=1.0)\n",
        "                except queue.Full:\n",
        "                    # If the result queue is full, drop the oldest result\n",
        "                    try:\n",
        "                        self.result_queue.get_nowait()\n",
        "                        self.result_queue.put(result_package, timeout=1.0)\n",
        "                    except (queue.Empty, queue.Full):\n",
        "                        pass\n",
        "\n",
        "                # Mark queue task as done\n",
        "                self.frame_queue.task_done()\n",
        "\n",
        "            except queue.Empty:\n",
        "                # No frames available, wait a bit\n",
        "                time.sleep(0.01)\n",
        "            except Exception as e:\n",
        "                print(f\"Error in detection worker: {e}\")\n",
        "                continue\n",
        "\n",
        "    def get_next_result(self, timeout=1.0):\n",
        "        \"\"\"Get the next processed frame and detection results\"\"\"\n",
        "        try:\n",
        "            return self.result_queue.get(timeout=timeout)\n",
        "        except queue.Empty:\n",
        "            return None\n",
        "\n",
        "class TrafficMonitoringSystem:\n",
        "    \"\"\"Integrated traffic monitoring system with all milestones and real-time optimization\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        # Update configuration with custom settings\n",
        "        self.config = CONFIG.copy()\n",
        "        if config:\n",
        "            self.config.update(config)\n",
        "\n",
        "        # Initialize components\n",
        "        self.performance = PerformanceTracker()\n",
        "        self.model = None\n",
        "        self.threaded_processor = None\n",
        "        self.tracker = None\n",
        "        self.video_writer = None\n",
        "\n",
        "        # Tracking data\n",
        "        self.trajectories = defaultdict(list)\n",
        "        self.smooth_trajectories = defaultdict(list)\n",
        "        self.speeds = defaultdict(float)\n",
        "        self.behaviors = defaultdict(str)\n",
        "        self.violations = defaultdict(bool)\n",
        "        self.is_visible = defaultdict(bool)\n",
        "        self.occlusion_stats = defaultdict(lambda: {\"count\": 0, \"total_frames\": 0, \"current_streak\": 0})\n",
        "\n",
        "        # Performance metrics\n",
        "        self.processing_times = []\n",
        "        self.frame_count = 0\n",
        "        self.total_vehicles = 0\n",
        "        self.total_unique_vehicles = 0\n",
        "        self.total_violations = 0\n",
        "\n",
        "        # Visualization settings\n",
        "        self.colors = np.random.randint(0, 255, size=(100, 3), dtype=np.uint8)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load the YOLOv5 model with optimizations\"\"\"\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        self.performance.start(\"model_loading\")\n",
        "\n",
        "        # Load the model\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
        "\n",
        "        # Apply optimizations\n",
        "        self.model.conf = self.config[\"confidence_threshold\"]  # Confidence threshold\n",
        "        self.model.iou = 0.45  # IoU threshold\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Apply quantization if enabled and using CPU\n",
        "        if self.config[\"use_quantization\"] and device.type == 'cpu':\n",
        "            print(\"Applying quantization for CPU inference...\")\n",
        "            try:\n",
        "                # Set model to eval mode\n",
        "                self.model.eval()\n",
        "\n",
        "                # Apply static quantization to applicable parts\n",
        "                quantized_model = torch.quantization.quantize_dynamic(\n",
        "                    self.model,\n",
        "                    {torch.nn.Linear, torch.nn.Conv2d},\n",
        "                    dtype=torch.qint8\n",
        "                )\n",
        "\n",
        "                self.model = quantized_model\n",
        "                print(\"Model quantized successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Quantization failed: {e}\")\n",
        "\n",
        "        load_time = self.performance.stop(\"model_loading\")\n",
        "        print(f\"Model loaded in {load_time:.2f}s\")\n",
        "        return self.model\n",
        "\n",
        "    def initialize_tracker(self):\n",
        "        \"\"\"Initialize the DeepSORT tracker with optimized settings\"\"\"\n",
        "        self.tracker = DeepSort(\n",
        "            max_age=30,               # Maximum frames to keep track\n",
        "            n_init=3,                 # Frames before a track is confirmed\n",
        "            nn_budget=100,            # Maximum appearance features to store\n",
        "            embedder=\"mobilenet\",     # Feature extractor\n",
        "            nms_max_overlap=0.8 if self.config[\"enable_occlusion_handling\"] else 1.0,\n",
        "            max_cosine_distance=0.2,\n",
        "            max_iou_distance=0.7 if self.config[\"enable_occlusion_handling\"] else 0.7\n",
        "        )\n",
        "        return self.tracker\n",
        "\n",
        "    def initialize_video_processor(self, video_path):\n",
        "        \"\"\"Initialize the video processor (threaded or synchronous)\"\"\"\n",
        "        if self.config[\"use_threading\"]:\n",
        "            self.threaded_processor = ThreadedVideoProcessor(\n",
        "                video_path, self.model, buffer_size=30\n",
        "            )\n",
        "            self.threaded_processor.start()\n",
        "        else:\n",
        "            # Just open the video for synchronous processing\n",
        "            self.cap = cv2.VideoCapture(video_path)\n",
        "            if not self.cap.isOpened():\n",
        "                print(f\"Error: Could not open video {video_path}\")\n",
        "                return False\n",
        "\n",
        "            # Get video properties\n",
        "            self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
        "            self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        return True\n",
        "\n",
        "    def initialize_video_writer(self, output_path):\n",
        "        \"\"\"Initialize video writer if saving output\"\"\"\n",
        "        if self.config[\"save_output\"]:\n",
        "            # Use video properties from threaded processor or direct capture\n",
        "            if self.config[\"use_threading\"]:\n",
        "                fps = self.threaded_processor.fps\n",
        "                frame_width = self.threaded_processor.frame_width\n",
        "                frame_height = self.threaded_processor.frame_height\n",
        "            else:\n",
        "                fps = self.fps\n",
        "                frame_width = self.frame_width\n",
        "                frame_height = self.frame_height\n",
        "\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            self.video_writer = cv2.VideoWriter(\n",
        "                output_path, fourcc, fps, (frame_width, frame_height)\n",
        "            )\n",
        "            print(f\"Video output will be saved to {output_path}\")\n",
        "\n",
        "        return self.video_writer is not None if self.config[\"save_output\"] else True\n",
        "\n",
        "    def initialize(self, video_path, model_path, output_path=None):\n",
        "        \"\"\"Initialize all components of the system\"\"\"\n",
        "        print(\"Initializing Traffic Monitoring System...\")\n",
        "\n",
        "        # Load model\n",
        "        self.load_model(model_path)\n",
        "\n",
        "        # Initialize tracker\n",
        "        self.initialize_tracker()\n",
        "\n",
        "        # Initialize video processor\n",
        "        if not self.initialize_video_processor(video_path):\n",
        "            return False\n",
        "\n",
        "        # Initialize video writer if output path is provided\n",
        "        if output_path and self.config[\"save_output\"]:\n",
        "            self.initialize_video_writer(output_path)\n",
        "\n",
        "        print(\"Initialization complete\")\n",
        "        return True\n",
        "\n",
        "    def process_video(self):\n",
        "        \"\"\"Process the entire video\"\"\"\n",
        "        # Start the performance tracking\n",
        "        self.performance.start(\"total_processing\")\n",
        "\n",
        "        if self.config[\"use_threading\"]:\n",
        "            self._process_threaded()\n",
        "        else:\n",
        "            self._process_synchronous()\n",
        "\n",
        "        # Cleanup\n",
        "        if self.video_writer:\n",
        "            self.video_writer.release()\n",
        "\n",
        "        if not self.config[\"use_threading\"] and hasattr(self, 'cap'):\n",
        "            self.cap.release()\n",
        "\n",
        "        # Print performance summary\n",
        "        total_time = self.performance.stop(\"total_processing\")\n",
        "        print(f\"\\nProcessed {self.frame_count} frames in {total_time:.2f}s\")\n",
        "        print(f\"Average FPS: {self.frame_count / total_time:.2f}\")\n",
        "        print(f\"Total unique vehicles: {self.total_unique_vehicles}\")\n",
        "        print(f\"Total violations detected: {self.total_violations}\")\n",
        "\n",
        "        if self.config[\"enable_profiling\"]:\n",
        "            self.performance.print_summary()\n",
        "\n",
        "        return {\n",
        "            \"frames_processed\": self.frame_count,\n",
        "            \"total_time\": total_time,\n",
        "            \"avg_fps\": self.frame_count / total_time if total_time > 0 else 0,\n",
        "            \"unique_vehicles\": self.total_unique_vehicles,\n",
        "            \"violations\": self.total_violations\n",
        "        }\n",
        "\n",
        "    def _process_threaded(self):\n",
        "        \"\"\"Process video using threaded pipeline\"\"\"\n",
        "        frame_index = 0\n",
        "\n",
        "        while True:\n",
        "            # Check if we've reached the maximum frames\n",
        "            if self.config[\"max_frames\"] and frame_index >= self.config[\"max_frames\"]:\n",
        "                print(f\"Reached maximum frame limit ({self.config['max_frames']})\")\n",
        "                break\n",
        "\n",
        "            # Get the next result from the threaded processor\n",
        "            result_package = self.threaded_processor.get_next_result()\n",
        "\n",
        "            if result_package is None:\n",
        "                # No more frames or timeout\n",
        "                if not self.threaded_processor.running:\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            frame = result_package[\"frame\"]\n",
        "            results = result_package[\"results\"]\n",
        "            detection_fps = result_package[\"detection_fps\"]\n",
        "\n",
        "            # Process the detection results\n",
        "            self.performance.start(\"tracking\")\n",
        "            processed_frame = self._process_detections(frame, results)\n",
        "            tracking_time = self.performance.stop(\"tracking\")\n",
        "\n",
        "            # Calculate overall FPS\n",
        "            overall_fps = 1.0 / (tracking_time + result_package[\"detection_time\"])\n",
        "\n",
        "            # Add FPS info to the frame\n",
        "            cv2.putText(\n",
        "                processed_frame,\n",
        "                f\"FPS: {overall_fps:.1f} | Vehicles: {len(self.trajectories)} | Violations: {self.total_violations}\",\n",
        "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2\n",
        "            )\n",
        "\n",
        "            # Save frame to video if enabled\n",
        "            if self.video_writer:\n",
        "                self.video_writer.write(processed_frame)\n",
        "\n",
        "            # Display the frame if visualization is enabled\n",
        "            if self.config[\"display_results\"] and frame_index % self.config[\"show_every_nth_frame\"] == 0:\n",
        "                self._display_frame(processed_frame, frame_index, overall_fps)\n",
        "\n",
        "            # Update frame count\n",
        "            self.frame_count += 1\n",
        "            frame_index += 1\n",
        "\n",
        "        # Stop the threaded processor\n",
        "        self.threaded_processor.stop()\n",
        "\n",
        "    def _process_synchronous(self):\n",
        "        \"\"\"Process video synchronously (without threading)\"\"\"\n",
        "        frame_index = 0\n",
        "\n",
        "        while True:\n",
        "            # Check if we've reached the maximum frames\n",
        "            if self.config[\"max_frames\"] and frame_index >= self.config[\"max_frames\"]:\n",
        "                print(f\"Reached maximum frame limit ({self.config['max_frames']})\")\n",
        "                break\n",
        "\n",
        "            # Start timing this frame\n",
        "            self.performance.start(\"frame\")\n",
        "\n",
        "            # Read the next frame\n",
        "            self.performance.start(\"read\")\n",
        "            ret, frame = self.cap.read()\n",
        "            read_time = self.performance.stop(\"read\")\n",
        "\n",
        "            if not ret:\n",
        "                print(\"End of video\")\n",
        "                break\n",
        "\n",
        "            # Run detection\n",
        "            self.performance.start(\"detection\")\n",
        "            results = self.model(frame)\n",
        "            detection_time = self.performance.stop(\"detection\")\n",
        "\n",
        "            # Process detections and update tracking\n",
        "            self.performance.start(\"tracking\")\n",
        "            processed_frame = self._process_detections(frame, results)\n",
        "            tracking_time = self.performance.stop(\"tracking\")\n",
        "\n",
        "            # Calculate FPS\n",
        "            frame_time = self.performance.stop(\"frame\")\n",
        "            fps = 1.0 / frame_time if frame_time > 0 else 0\n",
        "\n",
        "            # Add FPS info to the frame\n",
        "            cv2.putText(\n",
        "                processed_frame,\n",
        "                f\"FPS: {fps:.1f} | Vehicles: {len(self.trajectories)} | Violations: {self.total_violations}\",\n",
        "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2\n",
        "            )\n",
        "\n",
        "            # Save frame to video if enabled\n",
        "            if self.video_writer:\n",
        "                self.video_writer.write(processed_frame)\n",
        "\n",
        "            # Display the frame if visualization is enabled\n",
        "            if self.config[\"display_results\"] and frame_index % self.config[\"show_every_nth_frame\"] == 0:\n",
        "                self._display_frame(processed_frame, frame_index, fps)\n",
        "\n",
        "            # Update frame count\n",
        "            self.frame_count += 1\n",
        "            frame_index += 1\n",
        "\n",
        "            # Store processing time for statistics\n",
        "            self.processing_times.append(frame_time)\n",
        "\n",
        "    def _process_detections(self, frame, results):\n",
        "        \"\"\"Process detection results and update tracking\"\"\"\n",
        "        # Get detections\n",
        "        detections = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "        # Format detections for DeepSORT\n",
        "        detection_list = []\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            detection_list.append(\n",
        "                ([x1, y1, x2 - x1, y2 - y1], conf, int(cls))\n",
        "            )\n",
        "\n",
        "        # Update tracker\n",
        "        tracks = self.tracker.update_tracks(detection_list, frame=frame)\n",
        "        active_tracks = [track for track in tracks if track.is_confirmed()]\n",
        "\n",
        "        # Count active tracks in this frame\n",
        "        frame_vehicle_count = len(active_tracks)\n",
        "        self.total_vehicles += frame_vehicle_count\n",
        "\n",
        "        # Create a copy of the frame for visualization\n",
        "        tracked_frame = frame.copy()\n",
        "\n",
        "        # Define intersection zones for behavior prediction (Milestone 3)\n",
        "        if self.config[\"enable_behavior_prediction\"]:\n",
        "            # Simple intersection zone definition - adjust based on your video\n",
        "            intersection = self._define_intersection_zones(frame.shape[1], frame.shape[0])\n",
        "            self._draw_intersection_zones(tracked_frame, intersection)\n",
        "\n",
        "        # Update visibility status for all tracks\n",
        "        current_track_ids = set()\n",
        "\n",
        "        # Track IDs that were visible in previous frame but not in current frame\n",
        "        if self.config[\"enable_occlusion_handling\"]:\n",
        "            previously_visible = set(track_id for track_id, visible in self.is_visible.items() if visible)\n",
        "\n",
        "        # Process each active track\n",
        "        for track in active_tracks:\n",
        "            track_id = track.track_id\n",
        "            current_track_ids.add(track_id)\n",
        "\n",
        "            # Mark track as visible\n",
        "            if self.config[\"enable_occlusion_handling\"]:\n",
        "                self.is_visible[track_id] = True\n",
        "                # Reset occlusion streak counter\n",
        "                self.occlusion_stats[track_id][\"current_streak\"] = 0\n",
        "\n",
        "            bbox = track.to_ltrb()\n",
        "            x1, y1, x2, y2 = bbox\n",
        "\n",
        "            # Get track position (center of bounding box)\n",
        "            center_x = int((x1 + x2) / 2)\n",
        "            center_y = int((y1 + y2) / 2)\n",
        "            current_pos = (center_x, center_y)\n",
        "\n",
        "            # Add point to trajectory\n",
        "            self.trajectories[track_id].append(current_pos)\n",
        "\n",
        "            # Update smooth trajectory if occlusion handling is enabled\n",
        "            if self.config[\"enable_occlusion_handling\"] and len(self.trajectories[track_id]) > 5:\n",
        "                self.smooth_trajectories[track_id] = self._smooth_trajectory(self.trajectories[track_id])\n",
        "            else:\n",
        "                self.smooth_trajectories[track_id] = self.trajectories[track_id].copy()\n",
        "\n",
        "            # Update speed estimate\n",
        "            if hasattr(self, 'fps'):\n",
        "                self.speeds[track_id] = self._estimate_speed(self.trajectories[track_id], self.fps)\n",
        "\n",
        "            # Count unique vehicles\n",
        "            if len(self.trajectories[track_id]) == 1:\n",
        "                self.total_unique_vehicles += 1\n",
        "\n",
        "            # Analyze behavior if enabled (Milestone 3)\n",
        "            if self.config[\"enable_behavior_prediction\"] and len(self.trajectories[track_id]) >= 10:\n",
        "                # Determine which region the vehicle is in\n",
        "                if center_x < intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                    approach_direction = 'north'\n",
        "                elif center_x >= intersection['center'][0] and center_y < intersection['center'][1]:\n",
        "                    approach_direction = 'east'\n",
        "                elif center_x < intersection['center'][0] and center_y >= intersection['center'][1]:\n",
        "                    approach_direction = 'west'\n",
        "                else:\n",
        "                    approach_direction = 'south'\n",
        "\n",
        "                # Update behavior\n",
        "                self.behaviors[track_id] = self._analyze_trajectory_direction(self.trajectories[track_id])\n",
        "\n",
        "                # Check for red light violation\n",
        "                stop_line_y = intersection['stop_lines'][approach_direction]\n",
        "                light_state = intersection['light_states'][approach_direction]\n",
        "\n",
        "                # Detect violation\n",
        "                new_violation = self._detect_red_light_violation(\n",
        "                    self.trajectories[track_id],\n",
        "                    current_pos,\n",
        "                    stop_line_y,\n",
        "                    light_state\n",
        "                )\n",
        "\n",
        "                # Count new violations\n",
        "                if new_violation and not self.violations[track_id]:\n",
        "                    self.total_violations += 1\n",
        "\n",
        "                self.violations[track_id] = new_violation\n",
        "\n",
        "            # Get color for this track ID\n",
        "            color = self.colors[track_id % len(self.colors)].tolist()\n",
        "\n",
        "            # Draw bounding box (red for violation, normal color otherwise)\n",
        "            box_color = (0, 0, 255) if self.violations.get(track_id, False) else color\n",
        "            cv2.rectangle(tracked_frame, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)\n",
        "\n",
        "            # Add ID and info text\n",
        "            info_text = f\"ID:{track_id}\"\n",
        "            if self.config[\"enable_behavior_prediction\"]:\n",
        "                info_text += f\" {self.behaviors.get(track_id, '')}\"\n",
        "            cv2.putText(tracked_frame, info_text, (int(x1), int(y1)-10),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Add speed text\n",
        "            cv2.putText(tracked_frame, f\"Speed:{self.speeds.get(track_id, 0):.1f}\", (int(x1), int(y1)-30),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Add violation warning\n",
        "            if self.violations.get(track_id, False):\n",
        "                cv2.putText(tracked_frame, \"VIOLATION!\", (int(x1), int(y1)-50),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "            # Draw trajectory\n",
        "            if len(self.smooth_trajectories[track_id]) > 1:\n",
        "                # Draw the trajectory line (last 20 points max for efficiency)\n",
        "                traj_points = self.smooth_trajectories[track_id][-20:]\n",
        "                for i in range(1, len(traj_points)):\n",
        "                    pt1 = tuple(map(int, traj_points[i-1]))\n",
        "                    pt2 = tuple(map(int, traj_points[i]))\n",
        "                    cv2.line(tracked_frame, pt1, pt2, color, 2)\n",
        "\n",
        "        # Handle occlusions if enabled (Milestone 4)\n",
        "        if self.config[\"enable_occlusion_handling\"]:\n",
        "            # Find tracks that are occluded in current frame\n",
        "            all_known_tracks = set(self.trajectories.keys())\n",
        "            occluded_tracks = all_known_tracks - current_track_ids\n",
        "\n",
        "            for track_id in occluded_tracks:\n",
        "                # Skip if track has been missing for too long\n",
        "                if self.frame_count - len(self.trajectories[track_id]) > 30:  # max_age\n",
        "                    continue\n",
        "\n",
        "                # Track is occluded in this frame\n",
        "                if self.is_visible[track_id]:  # First frame of occlusion\n",
        "                    self.is_visible[track_id] = False\n",
        "                    self.occlusion_stats[track_id][\"count\"] += 1\n",
        "\n",
        "                # Update occlusion streak counter\n",
        "                self.occlusion_stats[track_id][\"current_streak\"] += 1\n",
        "                self.occlusion_stats[track_id][\"total_frames\"] += 1\n",
        "\n",
        "                # Predict position during occlusion\n",
        "                if len(self.trajectories[track_id]) > 5:\n",
        "                    # Simple linear prediction based on last few positions\n",
        "                    last_positions = self.trajectories[track_id][-5:]\n",
        "                    dx = np.mean([last_positions[i][0] - last_positions[i-1][0] for i in range(1, 5)])\n",
        "                    dy = np.mean([last_positions[i][1] - last_positions[i-1][1] for i in range(1, 5)])\n",
        "\n",
        "                    last_pos = last_positions[-1]\n",
        "                    predicted_pos = (int(last_pos[0] + dx), int(last_pos[1] + dy))\n",
        "\n",
        "                    # Add predicted position to trajectory\n",
        "                    self.trajectories[track_id].append(predicted_pos)\n",
        "\n",
        "                    # Update smoothed trajectory\n",
        "                    self.smooth_trajectories[track_id] = self._smooth_trajectory(self.trajectories[track_id])\n",
        "\n",
        "                    # Draw occluded bounding box with dashed lines\n",
        "                    box_width, box_height = 60, 60  # Approximate size\n",
        "                    cv2.rectangle(\n",
        "                        tracked_frame,\n",
        "                        (predicted_pos[0] - box_width//2, predicted_pos[1] - box_height//2),\n",
        "                        (predicted_pos[0] + box_width//2, predicted_pos[1] + box_height//2),\n",
        "                        (255, 255, 0), 1, cv2.LINE_DASHED\n",
        "                    )\n",
        "\n",
        "                    # Draw text\n",
        "                    cv2.putText(\n",
        "                        tracked_frame,\n",
        "                        f\"Occluded ID:{track_id}\",\n",
        "                        (predicted_pos[0] - box_width//2, predicted_pos[1] - box_height//2 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2\n",
        "                    )\n",
        "\n",
        "        return tracked_frame\n",
        "\n",
        "    def _display_frame(self, frame, frame_index, fps):\n",
        "        \"\"\"Display a processed frame\"\"\"\n",
        "        # Convert BGR to RGB for matplotlib\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(frame_rgb)\n",
        "        plt.title(f\"Frame {frame_index} | FPS: {fps:.2f} | Vehicles: {self.total_unique_vehicles} | Violations: {self.total_violations}\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Helper functions below\n",
        "\n",
        "    def _estimate_speed(self, traj_points, fps):\n",
        "        \"\"\"Estimate vehicle speed based on trajectory points\"\"\"\n",
        "        if len(traj_points) < 5:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate displacement over last 5 points\n",
        "        distances = []\n",
        "        for i in range(1, min(6, len(traj_points))):\n",
        "            distances.append(self._calculate_distance(traj_points[-i], traj_points[-i-1]))\n",
        "\n",
        "        # Average displacement per frame\n",
        "        avg_displacement = sum(distances) / len(distances)\n",
        "\n",
        "        # Convert to speed (pixels per second)\n",
        "        speed = avg_displacement * fps\n",
        "\n",
        "        return speed\n",
        "\n",
        "    def _calculate_distance(self, point1, point2):\n",
        "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
        "        return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
        "\n",
        "    def _smooth_trajectory(self, trajectory, window_size=15, poly_order=3):\n",
        "        \"\"\"Smooth a trajectory using Savitzky-Golay filter\"\"\"\n",
        "        if len(trajectory) < window_size:\n",
        "            return trajectory\n",
        "\n",
        "        # Convert to numpy array\n",
        "        traj_np = np.array(trajectory)\n",
        "\n",
        "        # Adjust window size if needed\n",
        "        if window_size % 2 == 0:\n",
        "            window_size -= 1\n",
        "\n",
        "        if window_size > len(traj_np):\n",
        "            window_size = len(traj_np) - 1\n",
        "            if window_size % 2 == 0:\n",
        "                window_size -= 1\n",
        "\n",
        "        if window_size < poly_order + 1:\n",
        "            return trajectory\n",
        "\n",
        "        try:\n",
        "            # Smooth x and y separately\n",
        "            x_smooth = savgol_filter(traj_np[:, 0], window_size, poly_order)\n",
        "            y_smooth = savgol_filter(traj_np[:, 1], window_size, poly_order)\n",
        "\n",
        "            # Combine back to pairs\n",
        "            smoothed_traj = list(zip(x_smooth, y_smooth))\n",
        "            return smoothed_traj\n",
        "        except Exception as e:\n",
        "            print(f\"Smoothing error: {e}\")\n",
        "            return trajectory\n",
        "\n",
        "    def _analyze_trajectory_direction(self, trajectory, window_size=10):\n",
        "        \"\"\"Analyze trajectory to determine if vehicle is turning and in which direction\"\"\"\n",
        "        if len(trajectory) < window_size:\n",
        "            return \"Unknown\"\n",
        "\n",
        "        # Get recent trajectory points\n",
        "        recent_traj = trajectory[-window_size:]\n",
        "\n",
        "        # Calculate direction changes\n",
        "        angles = []\n",
        "        for i in range(1, len(recent_traj)):\n",
        "            dx = recent_traj[i][0] - recent_traj[i-1][0]\n",
        "            dy = recent_traj[i][1] - recent_traj[i-1][1]\n",
        "            angle = math.atan2(dy, dx)\n",
        "            angles.append(angle)\n",
        "\n",
        "        # Calculate angle difference between start and end\n",
        "        angle_diff = abs(angles[-1] - angles[0])\n",
        "\n",
        "        # Classify turn direction\n",
        "        if angle_diff > 0.3:  # Threshold for turn detection (in radians)\n",
        "            # Determine turn direction\n",
        "            if (angles[-1] - angles[0]) > 0:\n",
        "                return \"Turning Left\"\n",
        "            else:\n",
        "                return \"Turning Right\"\n",
        "        else:\n",
        "            return \"Going Straight\"\n",
        "\n",
        "    def _detect_red_light_violation(self, trajectory, current_pos, stop_line_y, light_state):\n",
        "        \"\"\"Detect if a vehicle might run a red light\"\"\"\n",
        "        if light_state != \"RED\" or len(trajectory) < 5:\n",
        "            return False\n",
        "\n",
        "        # Check if vehicle is approaching the stop line\n",
        "        if current_pos[1] < stop_line_y and current_pos[1] > stop_line_y - 100:\n",
        "            # Calculate speed\n",
        "            recent_traj = trajectory[-5:]\n",
        "            distances = []\n",
        "            for i in range(1, len(recent_traj)):\n",
        "                distances.append(self._calculate_distance(recent_traj[i], recent_traj[i-1]))\n",
        "\n",
        "            avg_speed = sum(distances) / len(distances)\n",
        "\n",
        "            # If approaching fast and not slowing down\n",
        "            if avg_speed > 5.0:  # Threshold for \"fast\" approach\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _define_intersection_zones(self, frame_width, frame_height):\n",
        "        \"\"\"Define virtual zones for traffic analysis based on frame dimensions\"\"\"\n",
        "        # Example for a four-way intersection\n",
        "        intersection = {\n",
        "            'center': (frame_width//2, frame_height//2),\n",
        "            'radius': 150,\n",
        "            'stop_lines': {\n",
        "                'north': frame_height//2 - 100,\n",
        "                'south': frame_height//2 + 100,\n",
        "                'east': frame_width//2 + 100,\n",
        "                'west': frame_width//2 - 100\n",
        "            },\n",
        "            'light_states': {\n",
        "                'north': 'RED',\n",
        "                'south': 'RED',\n",
        "                'east': 'GREEN',\n",
        "                'west': 'RED'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return intersection\n",
        "\n",
        "    def _draw_intersection_zones(self, frame, intersection):\n",
        "        \"\"\"Draw intersection zones on the frame\"\"\"\n",
        "        frame_width = frame.shape[1]\n",
        "        frame_height = frame.shape[0]\n",
        "\n",
        "        # Draw center circle\n",
        "        cv2.circle(frame, intersection['center'], 10, (0, 0, 255), -1)\n",
        "        cv2.circle(frame, intersection['center'], intersection['radius'], (0, 0, 255), 2)\n",
        "\n",
        "        # Draw stop lines\n",
        "        cv2.line(frame,\n",
        "                 (0, intersection['stop_lines']['north']),\n",
        "                 (frame_width, intersection['stop_lines']['north']),\n",
        "                 (255, 0, 0), 2)\n",
        "        cv2.line(frame,\n",
        "                 (0, intersection['stop_lines']['south']),\n",
        "                 (frame_width, intersection['stop_lines']['south']),\n",
        "                 (255, 0, 0), 2)\n",
        "        cv2.line(frame,\n",
        "                 (intersection['stop_lines']['west'], 0),\n",
        "                 (intersection['stop_lines']['west'], frame_height),\n",
        "                 (255, 0, 0), 2)\n",
        "        cv2.line(frame,\n",
        "                 (intersection['stop_lines']['east'], 0),\n",
        "                 (intersection['stop_lines']['east'], frame_height),\n",
        "                 (255, 0, 0), 2)\n",
        "\n",
        "        # Add traffic light indicators\n",
        "        light_positions = {\n",
        "            'north': (frame_width//2 - 50, intersection['stop_lines']['north'] - 20),\n",
        "            'south': (frame_width//2 + 50, intersection['stop_lines']['south'] + 20),\n",
        "            'east': (intersection['stop_lines']['east'] + 20, frame_height//2 - 50),\n",
        "            'west': (intersection['stop_lines']['west'] - 20, frame_height//2 + 50)\n",
        "        }\n",
        "\n",
        "        for direction, position in light_positions.items():\n",
        "            # Determine light color\n",
        "            if intersection['light_states'][direction] == 'RED':\n",
        "                light_color = (0, 0, 255)  # Red\n",
        "            elif intersection['light_states'][direction] == 'YELLOW':\n",
        "                light_color = (0, 255, 255)  # Yellow\n",
        "            else:  # GREEN\n",
        "                light_color = (0, 255, 0)  # Green\n",
        "\n",
        "            # Draw light indicator\n",
        "            cv2.circle(frame, position, 15, light_color, -1)\n",
        "            cv2.circle(frame, position, 15, (255, 255, 255), 2)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def _update_traffic_lights(self, intersection, frame_count, cycle_length=150):\n",
        "        \"\"\"Update traffic light states based on a simple cycle\"\"\"\n",
        "        # Simple traffic light cycle: N/S green, E/W red, then switch\n",
        "        cycle_position = frame_count % cycle_length\n",
        "\n",
        "        # First half of cycle: N/S green, E/W red\n",
        "        if cycle_position < cycle_length // 2:\n",
        "            intersection['light_states']['north'] = 'GREEN'\n",
        "            intersection['light_states']['south'] = 'GREEN'\n",
        "            intersection['light_states']['east'] = 'RED'\n",
        "            intersection['light_states']['west'] = 'RED'\n",
        "\n",
        "            # Add yellow transition\n",
        "            if cycle_position > (cycle_length // 2) - 30:\n",
        "                intersection['light_states']['north'] = 'YELLOW'\n",
        "                intersection['light_states']['south'] = 'YELLOW'\n",
        "        # Second half: N/S red, E/W green\n",
        "        else:\n",
        "            intersection['light_states']['north'] = 'RED'\n",
        "            intersection['light_states']['south'] = 'RED'\n",
        "            intersection['light_states']['east'] = 'GREEN'\n",
        "            intersection['light_states']['west'] = 'GREEN'\n",
        "\n",
        "            # Add yellow transition\n",
        "            if cycle_position > cycle_length - 30:\n",
        "                intersection['light_states']['east'] = 'YELLOW'\n",
        "                intersection['light_states']['west'] = 'YELLOW'\n",
        "\n",
        "        return intersection\n",
        "\n",
        "# Main function to run the traffic monitoring system\n",
        "def run_traffic_monitoring(\n",
        "    video_path,\n",
        "    model_path,\n",
        "    output_path=None,\n",
        "    use_threading=True,\n",
        "    use_quantization=True,\n",
        "    max_frames=None,\n",
        "    show_visualization=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the complete traffic monitoring system with all optimizations.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the input video\n",
        "        model_path: Path to the YOLOv5 model weights\n",
        "        output_path: Path to save the output video (None = no output)\n",
        "        use_threading: Enable multi-threading for better performance\n",
        "        use_quantization: Enable model quantization for faster CPU inference\n",
        "        max_frames: Maximum number of frames to process (None = all frames)\n",
        "        show_visualization: Whether to display visualizations\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with processing statistics\n",
        "    \"\"\"\n",
        "    # Configure the system\n",
        "    config = {\n",
        "        \"use_threading\": use_threading,\n",
        "        \"use_quantization\": use_quantization,\n",
        "        \"display_results\": show_visualization,\n",
        "        \"save_output\": output_path is not None,\n",
        "        \"max_frames\": max_frames,\n",
        "        \"enable_behavior_prediction\": True,\n",
        "        \"enable_occlusion_handling\": True,\n",
        "        \"enable_profiling\": True\n",
        "    }\n",
        "\n",
        "    # Initialize the system\n",
        "    system = TrafficMonitoringSystem(config)\n",
        "\n",
        "    # Start the system\n",
        "    print(f\"Starting traffic monitoring on {video_path}\")\n",
        "    print(f\"Model: {model_path}\")\n",
        "    print(f\"Configuration: threading={use_threading}, quantization={use_quantization}\")\n",
        "\n",
        "    if not system.initialize(video_path, model_path, output_path):\n",
        "        print(\"Initialization failed.\")\n",
        "        return None\n",
        "\n",
        "    # Process the video\n",
        "    results = system.process_video()\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    if output_path and config[\"save_output\"]:\n",
        "        print(f\"Output video saved to: {output_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Command-line interface\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Optimized Traffic Monitoring System\")\n",
        "    parser.add_argument(\"--video\", type=str, required=True, help=\"Path to input video\")\n",
        "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to YOLOv5 model weights\")\n",
        "    parser.add_argument(\"--output\", type=str, default=None, help=\"Path to save output video\")\n",
        "    parser.add_argument(\"--no-threading\", action=\"store_true\", help=\"Disable multi-threading\")\n",
        "    parser.add_argument(\"--no-quantization\", action=\"store_true\", help=\"Disable model quantization\")\n",
        "    parser.add_argument(\"--max-frames\", type=int, default=None, help=\"Maximum frames to process\")\n",
        "    parser.add_argument(\"--no-visualization\", action=\"store_true\", help=\"Disable visualization\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Run the system\n",
        "    run_traffic_monitoring(\n",
        "        video_path=args.video,\n",
        "        model_path=args.model,\n",
        "        output_path=args.output,\n",
        "        use_threading=not args.no_threading,\n",
        "        use_quantization=not args.no_quantization,\n",
        "        max_frames=args.max_frames,\n",
        "        show_visualization=not args.no_visualization\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FKVgw8OHwWE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Traffic Monitoring Demo\n",
        "# This script demonstrates the full traffic monitoring system with real-time optimization\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Import our system\n",
        "from traffic_monitoring_final import TrafficMonitoringSystem, run_traffic_monitoring\n",
        "\n",
        "# Ensure the required packages are installed\n",
        "try:\n",
        "    import deep_sort_realtime\n",
        "except ImportError:\n",
        "    print(\"Installing deep-sort-realtime...\")\n",
        "    !pip install -q deep-sort-realtime\n",
        "\n",
        "try:\n",
        "    from scipy.signal import savgol_filter\n",
        "except ImportError:\n",
        "    print(\"Installing scipy...\")\n",
        "    !pip install -q scipy\n",
        "\n",
        "# Set the paths to your files\n",
        "# Adjust these paths to match your environment\n",
        "VIDEO_PATH = \"/root/.cache/kagglehub/datasets/raguhudarare/videotraffic/versions/2/Traffic.mp4\"\n",
        "MODEL_PATH = \"runs/train/exp/weights/best.pt\"  # Path to your trained model\n",
        "OUTPUT_PATH = \"traffic_monitoring_output.mp4\"\n",
        "\n",
        "# Check if video exists\n",
        "if not os.path.exists(VIDEO_PATH):\n",
        "    print(f\"Video not found at {VIDEO_PATH}\")\n",
        "    print(\"Searching for alternative video paths...\")\n",
        "\n",
        "    # Try some common alternatives\n",
        "    alternatives = [\n",
        "        \"/kaggle/input/videotraffic/Traffic.mp4\",\n",
        "        \"../input/videotraffic/Traffic.mp4\",\n",
        "        \"Traffic.mp4\"\n",
        "    ]\n",
        "\n",
        "    for alt_path in alternatives:\n",
        "        if os.path.exists(alt_path):\n",
        "            VIDEO_PATH = alt_path\n",
        "            print(f\"Found video at {VIDEO_PATH}\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"No suitable video found. Please update the VIDEO_PATH variable.\")\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"Model not found at {MODEL_PATH}\")\n",
        "    print(\"Searching for alternative model paths...\")\n",
        "\n",
        "    # Try some common alternatives\n",
        "    alternatives = [\n",
        "        \"runs/train/exp3/weights/best.pt\",\n",
        "        \"/kaggle/working/yolov5/runs/train/exp/weights/best.pt\",\n",
        "        \"yolov5s.pt\"  # Fall back to pre-trained model\n",
        "    ]\n",
        "\n",
        "    for alt_path in alternatives:\n",
        "        if os.path.exists(alt_path):\n",
        "            MODEL_PATH = alt_path\n",
        "            print(f\"Found model at {MODEL_PATH}\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"No suitable model found. Will try to download YOLOv5s...\")\n",
        "        try:\n",
        "            # Try to download the pre-trained YOLOv5s model\n",
        "            model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "            MODEL_PATH = \"yolov5s.pt\"\n",
        "            print(\"Downloaded pre-trained YOLOv5s model\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download model: {e}\")\n",
        "            print(\"Please update the MODEL_PATH variable.\")\n",
        "\n",
        "# Function to run benchmark of different configurations\n",
        "def run_benchmark():\n",
        "    print(\"\\n===== Running Benchmark =====\")\n",
        "\n",
        "    configs = [\n",
        "        {\"name\": \"Baseline\", \"threading\": False, \"quantization\": False},\n",
        "        {\"name\": \"Threading Only\", \"threading\": True, \"quantization\": False},\n",
        "        {\"name\": \"Quantization Only\", \"threading\": False, \"quantization\": True},\n",
        "        {\"name\": \"Full Optimization\", \"threading\": True, \"quantization\": True}\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Process 100 frames with each configuration\n",
        "    for config in configs:\n",
        "        print(f\"\\nTesting: {config['name']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the system with this configuration\n",
        "        stats = run_traffic_monitoring(\n",
        "            video_path=VIDEO_PATH,\n",
        "            model_path=MODEL_PATH,\n",
        "            output_path=None,  # Don't save output for benchmark\n",
        "            use_threading=config[\"threading\"],\n",
        "            use_quantization=config[\"quantization\"],\n",
        "            max_frames=100,\n",
        "            show_visualization=False  # Disable visualization for benchmark\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        if stats:\n",
        "            results.append({\n",
        "                \"name\": config[\"name\"],\n",
        "                \"fps\": stats[\"avg_fps\"],\n",
        "                \"time\": elapsed,\n",
        "                \"config\": config\n",
        "            })\n",
        "\n",
        "    # Display results\n",
        "    if results:\n",
        "        print(\"\\n===== Benchmark Results =====\")\n",
        "        print(\"Configuration | Avg FPS | Total Time (s)\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for result in results:\n",
        "            print(f\"{result['name']:<15} | {result['fps']:<7.2f} | {result['time']:<12.2f}\")\n",
        "\n",
        "        # Visualize results\n",
        "        names = [r[\"name\"] for r in results]\n",
        "        fps_values = [r[\"fps\"] for r in results]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(names, fps_values, color='skyblue')\n",
        "        plt.title('Performance Comparison (FPS)')\n",
        "        plt.ylabel('Frames Per Second')\n",
        "        plt.axhline(y=30, color='r', linestyle='--', label='Real-Time (30 FPS)')\n",
        "        plt.axhline(y=24, color='g', linestyle='--', label='Smooth Video (24 FPS)')\n",
        "        plt.legend()\n",
        "\n",
        "        # Add FPS values on top of bars\n",
        "        for i, v in enumerate(fps_values):\n",
        "            plt.text(i, v + 0.5, f\"{v:.1f}\", ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Return the best configuration\n",
        "        best_result = max(results, key=lambda x: x[\"fps\"])\n",
        "        print(f\"\\nBest configuration: {best_result['name']} with {best_result['fps']:.2f} FPS\")\n",
        "        return best_result[\"config\"]\n",
        "\n",
        "    # Default configuration if benchmark fails\n",
        "    return {\"threading\": True, \"quantization\": True}\n",
        "\n",
        "# Run the full demo\n",
        "def run_full_demo(use_benchmark=True):\n",
        "    print(\"\\n===== Traffic Monitoring System Demo =====\")\n",
        "\n",
        "    if use_benchmark:\n",
        "        print(\"Running benchmark to find optimal configuration...\")\n",
        "        best_config = run_benchmark()\n",
        "        use_threading = best_config[\"threading\"]\n",
        "        use_quantization = best_config[\"quantization\"]\n",
        "\n",
        "        print(f\"\\nUsing optimal configuration: Threading={use_threading}, Quantization={use_quantization}\")\n",
        "    else:\n",
        "        # Use default configuration\n",
        "        use_threading = True\n",
        "        use_quantization = True\n",
        "        print(\"Using default configuration (Threading=True, Quantization=True)\")\n",
        "\n",
        "    print(\"\\nRunning full traffic monitoring system...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Run the system with the chosen configuration\n",
        "    stats = run_traffic_monitoring(\n",
        "        video_path=VIDEO_PATH,\n",
        "        model_path=MODEL_PATH,\n",
        "        output_path=OUTPUT_PATH,\n",
        "        use_threading=use_threading,\n",
        "        use_quantization=use_quantization,\n",
        "        max_frames=None,  # Process the entire video\n",
        "        show_visualization=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n===== Demo Complete =====\")\n",
        "    if stats:\n",
        "        print(f\"Processed {stats['frames_processed']} frames at {stats['avg_fps']:.2f} FPS\")\n",
        "        print(f\"Detected {stats['unique_vehicles']} unique vehicles\")\n",
        "        print(f\"Detected {stats['violations']} traffic violations\")\n",
        "\n",
        "    if os.path.exists(OUTPUT_PATH):\n",
        "        print(f\"Output video saved to: {OUTPUT_PATH}\")\n",
        "\n",
        "# Run the demo\n",
        "if __name__ == \"__main__\":\n",
        "    # Save the system implementation to a Python file\n",
        "    with open(\"traffic_monitoring_final.py\", \"w\") as f:\n",
        "        from IPython import get_ipython\n",
        "        code = get_ipython().user_ns.get(\"_\")\n",
        "        if code:\n",
        "            f.write(code)\n",
        "        else:\n",
        "            print(\"Error: Could not get the code for the traffic monitoring system.\")\n",
        "            print(\"Please save the code to 'traffic_monitoring_final.py' manually.\")\n",
        "\n",
        "    # Run the demo with benchmark\n",
        "    run_full_demo(use_benchmark=True)"
      ],
      "metadata": {
        "id": "Hnt4fwYmweBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}